{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Automated Transformation and Optimization for Skewed and Imbalanced Datasets\n",
    "\n",
    "## Introduction\n",
    "This notebook integrates **data preprocessing, model optimization, and performance evaluation** to improve machine learning results on skewed and imbalanced datasets.\n",
    "**The pipeline is designed to be flexible and can be applied to multiple datasets.**\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:51:16.542257Z",
     "start_time": "2025-03-02T12:51:10.613371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ],
   "outputs": [],
   "execution_count": 584
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Integrating the Previous Project into the Current Research\n",
    "The previous project successfully demonstrated how class balancing and feature transformations impact model performance. These same principles are applied in the current research, but with a broader focus on automating preprocessing techniques for diverse datasets.\n",
    "\n",
    "- **Class Balancing** (SMOTE & Undersampling): In the wine quality dataset, balancing improved model accuracy by ensuring all classes were well represented. Similarly, in the current research, we apply controlled oversampling and undersampling to create a more robust classification pipeline.\n",
    "\n",
    "- **Skewness Correction**: The previous project applied log transformations to normalize distributions, preventing biases. Here, we expand this with Box-Cox and Yeo-Johnson transformations, adapting preprocessing to different data types.\n",
    "\n",
    "- **Model Optimization**: In the last project, hyperparameter tuning boosted performance dramatically. We extend this by optimizing classification thresholds and cost-sensitive learning to handle imbalanced datasets more effectively."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:51:16.554190Z",
     "start_time": "2025-03-02T12:51:16.545307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Load dataset\n",
    "# data = pd.read_csv(\"winequality-white.csv\", sep=';')\n",
    "#\n",
    "# # Visualize initial class distribution\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# sns.countplot(x='quality', data=data)\n",
    "# plt.title('Wine Quality Distribution - Before Balancing', fontsize=14)\n",
    "# plt.xlabel('Quality', fontsize=12)\n",
    "# plt.ylabel('Frequency', fontsize=12)\n",
    "# plt.show()\n",
    "#\n",
    "# # Split features and target\n",
    "# X = data.drop(columns=['quality'])\n",
    "# y = data['quality']\n",
    "#\n",
    "# # Display initial class distribution\n",
    "# print(\"Class Distribution Before Balancing:\", Counter(y))\n",
    "#\n",
    "# # Step 1: Handle Extremely Rare Classes (Manual Duplication)\n",
    "# min_samples = 30  # Minimum samples required for SMOTE to work\n",
    "#\n",
    "# for cls, count in Counter(y).items():\n",
    "#     if count < min_samples:\n",
    "#         extra_samples = data[data['quality'] == cls].sample(min_samples - count, replace=True)\n",
    "#         data = pd.concat([data, extra_samples])\n",
    "#\n",
    "# # Update X and y after rare class handling\n",
    "# X = data.drop(columns=['quality'])\n",
    "# y = data['quality']\n",
    "#\n",
    "# # Step 2: Apply SMOTE to Underrepresented Classes\n",
    "# max_class_count = max(Counter(y).values())\n",
    "# smote_strategy = {cls: max(count, int(0.6 * max_class_count)) for cls, count in Counter(y).items() if count < 0.6 * max_class_count}\n",
    "#\n",
    "# smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "# X_smote, y_smote = smote.fit_resample(X, y)\n",
    "#\n",
    "# # Display new class distribution after SMOTE\n",
    "# print(\"Class Distribution After SMOTE:\", Counter(y_smote))\n",
    "#\n",
    "# # Visualize class distribution after SMOTE\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# sns.countplot(x=y_smote)\n",
    "# plt.title('Wine Quality Distribution - After SMOTE', fontsize=14)\n",
    "# plt.xlabel('Quality', fontsize=12)\n",
    "# plt.ylabel('Frequency', fontsize=12)\n",
    "# plt.show()\n",
    "#\n",
    "# # Step 3: Apply Proper Undersampling for Overrepresented Classes\n",
    "# min_class_count = min(Counter(y_smote).values())  # Smallest class after SMOTE\n",
    "# undersample_strategy = {}\n",
    "#\n",
    "# for cls, count in Counter(y_smote).items():\n",
    "#     if count > min_class_count * 1.5:  # Reduce dominant classes but keep them slightly higher than others\n",
    "#         undersample_strategy[cls] = int(min_class_count * 1.5)  # Keep majority at 1.5x the smallest class\n",
    "#\n",
    "# # Apply undersampling\n",
    "# undersample = RandomUnderSampler(sampling_strategy=undersample_strategy, random_state=42)\n",
    "# X_final, y_final = undersample.fit_resample(X_smote, y_smote)\n",
    "#\n",
    "# # Display final class distribution after proper balancing\n",
    "# print(\"Final Class Distribution After SMOTE & Corrected Undersampling:\", Counter(y_final))\n",
    "#\n",
    "# # Visualizing target distribution after full balancing\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# sns.countplot(x=y_final)\n",
    "# plt.title('Wine Quality Distribution - After SMOTE & Corrected Undersampling', fontsize=14)\n",
    "# plt.xlabel('Quality', fontsize=12)\n",
    "# plt.ylabel('Frequency', fontsize=12)\n",
    "# plt.show()\n",
    "#\n"
   ],
   "outputs": [],
   "execution_count": 585
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### How This Improves Prediction?"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:51:16.565004Z",
     "start_time": "2025-03-02T12:51:16.556204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Load the baseline model\n",
    "# baseline_model = joblib.load('baseline_model.pkl')\n",
    "#\n",
    "# # Use the final balanced dataset\n",
    "# X = X_final\n",
    "# y = y_final\n",
    "#\n",
    "# # Split dataset into train and test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#\n",
    "# # Predict on the test set using the baseline model\n",
    "# y_pred_baseline = baseline_model.predict(X_test)\n",
    "#\n",
    "# # Calculate baseline performance metrics\n",
    "# r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "# mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
    "# rmse_baseline = np.sqrt(mse_baseline)\n",
    "#\n",
    "# print(f\"Baseline Model Performance:\\nR^2 Score: {r2_baseline:.4f}\\nMSE: {mse_baseline:.4f}\\nRMSE: {rmse_baseline:.4f}\")\n",
    "#\n",
    "# # Hyperparameter tuning with GridSearchCV for XGBoost\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'subsample': [0.8, 1.0]\n",
    "# }\n",
    "#\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=xgb.XGBRegressor(),\n",
    "#     param_grid=param_grid,\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     cv=5,\n",
    "#     verbose=1\n",
    "# )\n",
    "#\n",
    "# # Fit GridSearch on the balanced dataset\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# best_model = grid_search.best_estimator_\n",
    "# print(f'Best Parameters: {grid_search.best_params_}')\n",
    "#\n",
    "# # Evaluate the improved model\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# rmse = np.sqrt(mse)\n",
    "#\n",
    "# print(f'Improved Model Performance:\\nRMSE: {rmse:.4f}, R^2: {r2:.4f}')\n",
    "#\n",
    "# # Create a comparison table\n",
    "# comparison_df = pd.DataFrame({\n",
    "#     'Metric': ['R^2', 'MSE', 'RMSE'],\n",
    "#     'Baseline Model': [r2_baseline, mse_baseline, rmse_baseline],\n",
    "#     'Improved Model': [r2, mse, rmse]\n",
    "# })\n",
    "#\n",
    "# print(\"\\nComparison of Baseline and Improved Models:\")\n",
    "# print(comparison_df)\n",
    "#\n",
    "# # Plot comparison of R^2\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# bar_width = 0.35\n",
    "# indices = np.arange(len(comparison_df))\n",
    "#\n",
    "# plt.bar(indices, comparison_df['Baseline Model'], bar_width, label='Baseline Model', color='blue')\n",
    "# plt.bar(indices + bar_width, comparison_df['Improved Model'], bar_width, label='Improved Model', color='green')\n",
    "#\n",
    "# plt.xlabel('Metrics')\n",
    "# plt.ylabel('Scores')\n",
    "# plt.title('Comparison of Model Performance')\n",
    "# plt.xticks(indices + bar_width / 2, comparison_df['Metric'])\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 586
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The improved model shows a significant enhancement in predictive performance compared to the baseline model. The RÂ² score increased from 0.769 to 0.928, indicating that the new model explains much more variance in the data. The MSE dropped from 0.85 to 0.26, demonstrating a major reduction in prediction errors. Additionally, the RMSE decreased from 0.92 to 0.51, confirming that the model's predictions are more precise and stable.\n",
    "\n",
    "##### **These improvements highlight the impact of data balancing and hyperparameter tuning on predictive accuracy. By applying SMOTE & undersampling, the model now performs better across all quality levels, rather than favoring the dominant ones.**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Broader Impact: Scalable Preprocessing for Various Datasets\n",
    "While the previous project focused on a specific dataset (wine quality), the current research aims to generalize preprocessing techniques to work across multiple data types and domains. By automating class balancing and skew correction, we develop a versatile, adaptable pipeline that enhances machine learning models regardless of dataset characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install lightgbm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T12:51:27.543676Z",
     "start_time": "2025-03-02T12:51:16.568016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\guyha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\guyha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\guyha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from lightgbm) (2.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\guyha\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "execution_count": 587
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 0: Preprocessing the Dataset\n",
    "**Loads the dataset from the specified file path**\n",
    "\n",
    "    - Displays dataset structure and basic statistics."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:51:29.519788Z",
     "start_time": "2025-03-02T12:51:27.545992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(filepath,target_column):\n",
    "    \"\"\"\n",
    "    Loads the credit card fraud dataset, extracts numerical features, and returns\n",
    "    the DataFrame along with a list of numerical columns (excluding the target column).\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): Loaded dataset.\n",
    "        numerical_features (list): List of numerical feature column names (excluding the target).\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Define target column\n",
    "\n",
    "\n",
    "    # Extract numerical features excluding the target column\n",
    "    numerical_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    if target_column in numerical_features:\n",
    "        numerical_features.remove(target_column)\n",
    "\n",
    "    # Display dataset shape and first few rows\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    print(\"\\nSample Rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "    return df, numerical_features  # Return DataFrame and numerical feature names\n",
    "\n",
    "target_column = 'Class'\n",
    "df, numerical_features = load_data(\"creditcard.csv\",target_column)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (284807, 31)\n",
      "\n",
      "Sample Rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 588
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:51:29.532068Z",
     "start_time": "2025-03-02T12:51:29.521799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_dtypes = pd.DataFrame(df.dtypes).reset_index()\n",
    "df_dtypes.columns = ['Column', 'Data Type']\n",
    "print(df_dtypes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Column Data Type\n",
      "0     Time   float64\n",
      "1       V1   float64\n",
      "2       V2   float64\n",
      "3       V3   float64\n",
      "4       V4   float64\n",
      "5       V5   float64\n",
      "6       V6   float64\n",
      "7       V7   float64\n",
      "8       V8   float64\n",
      "9       V9   float64\n",
      "10     V10   float64\n",
      "11     V11   float64\n",
      "12     V12   float64\n",
      "13     V13   float64\n",
      "14     V14   float64\n",
      "15     V15   float64\n",
      "16     V16   float64\n",
      "17     V17   float64\n",
      "18     V18   float64\n",
      "19     V19   float64\n",
      "20     V20   float64\n",
      "21     V21   float64\n",
      "22     V22   float64\n",
      "23     V23   float64\n",
      "24     V24   float64\n",
      "25     V25   float64\n",
      "26     V26   float64\n",
      "27     V27   float64\n",
      "28     V28   float64\n",
      "29  Amount   float64\n",
      "30   Class     int64\n"
     ]
    }
   ],
   "execution_count": 589
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Detecting and Correcting Skewed Features"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:51:29.822640Z",
     "start_time": "2025-03-02T12:51:29.534077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def detect_skewed_features(df, numerical_features, skew_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detects numerical features with skewness above a given threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input dataset.\n",
    "        numerical_features (list): List of numerical feature column names.\n",
    "        skew_threshold (float): The threshold above which features are considered skewed.\n",
    "\n",
    "    Returns:\n",
    "        skewed_features (list): List of skewed numerical feature names.\n",
    "        skewness_table (pd.DataFrame): DataFrame with skewness values for all numerical features.\n",
    "    \"\"\"\n",
    "    print(\"Detecting skewed features...\")\n",
    "\n",
    "    # Compute skewness for numerical features\n",
    "    skewness_values = df[numerical_features].skew()\n",
    "\n",
    "    # Select features where absolute skewness is greater than the threshold\n",
    "    skewed_features = skewness_values[abs(skewness_values) > skew_threshold].index.tolist()\n",
    "\n",
    "    # Store skewness values in a DataFrame for reference\n",
    "    skewness_table = pd.DataFrame({\n",
    "        \"Feature\": numerical_features,\n",
    "        \"Skewness\": skewness_values\n",
    "    }).sort_values(by=\"Skewness\", ascending=False)\n",
    "\n",
    "    print(f\"Skewed Features (>|{skew_threshold}|): {skewed_features}\")\n",
    "\n",
    "    return skewed_features, skewness_table  # Return both the feature names and skewness values\n",
    "\n",
    "# Example usage:\n",
    "skewed_features, skewness_table = detect_skewed_features(df, numerical_features)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting skewed features...\n",
      "Skewed Features (>|0.5|): ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V12', 'V14', 'V16', 'V17', 'V20', 'V21', 'V23', 'V24', 'V26', 'V27', 'V28', 'Amount']\n"
     ]
    }
   ],
   "execution_count": 590
  },
  {
   "cell_type": "markdown",
   "source": [
    "We would like to identifies numerical features in the dataset that have a skewed distribution. Skewness measures how much a feature deviates from a normal (bell-shaped) distribution. We calculate the skewness of each numerical feature and filter out those with an absolute skewness greater than 0.5, which indicates moderate to high skewness. These identified features may require transformation to improve model performance by ensuring a more normal-like distribution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **1.1 Addressing the Issue of Worsened Skewness After Transformation**\n",
    "\n",
    "In some cases, applying transformations actually increased the skewness rather than reducing it.\n",
    "To fix this, we will implement a smarter transformation selection approach that avoids making skewness worse.\n",
    "\n",
    "Three Improvements to Prevent Skewness Worsening:\n",
    "1. Add a Post-Transformation Check:\n",
    "If the skewness increases instead of decreasing, we will revert to the original data.\n",
    "2. Choose a More Intelligent Transformation:\n",
    "If all values are strictly positive, we apply Box-Cox.\n",
    "If the feature contains negative values, we use Yeo-Johnson instead.\n",
    "We avoid log transformations automatically, as they may worsen skewness in extreme cases.\n",
    "3. Store the Original Skewness and Compare:\n",
    "After applying a transformation, we check if skewness improved.\n",
    "If the transformation made it worse, we undo the change and restore the original values.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def best_transform(feature_series):\n",
    "    \"\"\"\n",
    "    Try several transformations and select the one that yields the lowest absolute skewness.\n",
    "    Returns the transformed series, the name of the chosen method, the original skew, and the new skew.\n",
    "    \"\"\"\n",
    "    original_skew = feature_series.skew()\n",
    "    best_method = 'none'\n",
    "    best_skew = abs(original_skew)\n",
    "    best_transformed = feature_series.copy()\n",
    "\n",
    "    # Dictionary to store transformation results\n",
    "    transforms = {}\n",
    "\n",
    "    # Try Box-Cox (only if all values are positive)\n",
    "    if (feature_series > 0).all():\n",
    "        try:\n",
    "            transformed, _ = boxcox(feature_series + 1)\n",
    "            skew_val = pd.Series(transformed).skew()\n",
    "            transforms['boxcox'] = (transformed, abs(skew_val))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    # Try Log1p transform (with shift)\n",
    "    try:\n",
    "        transformed = np.log1p(feature_series - feature_series.min() + 1)\n",
    "        skew_val = pd.Series(transformed).skew()\n",
    "        transforms['log1p'] = (transformed, abs(skew_val))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # Try Yeo-Johnson\n",
    "    try:\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "        transformed = pt.fit_transform(feature_series.values.reshape(-1, 1)).flatten()\n",
    "        skew_val = pd.Series(transformed).skew()\n",
    "        transforms['yeo-johnson'] = (transformed, abs(skew_val))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # Try QuantileTransformer (maps data to a normal distribution)\n",
    "    try:\n",
    "        qt = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "        transformed = qt.fit_transform(feature_series.values.reshape(-1, 1)).flatten()\n",
    "        skew_val = pd.Series(transformed).skew()\n",
    "        transforms['quantile'] = (transformed, abs(skew_val))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # Compare all transformations and select the best one if it improves skewness\n",
    "    for method, (trans, skew_val) in transforms.items():\n",
    "        if skew_val < best_skew:\n",
    "            best_skew = skew_val\n",
    "            best_method = method\n",
    "            best_transformed = trans\n",
    "\n",
    "    return best_transformed, best_method, original_skew, pd.Series(best_transformed).skew()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T12:51:29.832568Z",
     "start_time": "2025-03-02T12:51:29.824648Z"
    }
   },
   "outputs": [],
   "execution_count": 591
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **The following transformations are applied based on methodologies discussed in the article \"Skewness Be Gone: Transformative Tricks for Data Scientists\" by Vinod Chugani.**\n",
    "   #### **Quantile Transformation**\n",
    "   - **How it works:**\n",
    "     - Maps the feature values to a specified distribution, such as normal, based on their quantiles.\n",
    "     - Effectively addresses skewness by distributing data points evenly across the chosen distribution.\n",
    "   - **Best for:** Severe skewness, non-Gaussian distributions, and handling outliers.\n",
    "\n",
    "   #### **Yeo-Johnson Transformation**\n",
    "   - **How it works:**\n",
    "     - A power transformation adaptable to both positive and non-positive data.\n",
    "     - Applies an optimal transformation parameter to stabilize variance and reduce skewness.\n",
    "   - **Best for:** Moderate skewness, features with zero or negative values, and when data includes negative or zero values.\n",
    "\n",
    "   #### **Log1p (Natural Log + 1) Transformation**\n",
    "   - **How it works:**\n",
    "     - Uses a logarithmic transformation to compress large values and reduce right-skewness.\n",
    "     - Applies the natural logarithm to each data point after adding one, effectively handling zero and positive values.\n",
    "   - **Best for:** Mild skewness, right-skewed data, and features where small values dominate.\n",
    "\n",
    "   #### Applying Data Transformations\n",
    "\n",
    "   To address skewness in our data, we applied several transformations as recommended in the article [\"Skewness Be Gone: Transformative Tricks for Data Scientists\" by Vinod Chugani](https://machinelearningmastery.com/skewness-be-gone-transformative-tricks-for-data-scientists/):\n",
    "\n",
    "   - **Log1p Transformation:** Suitable for mildly skewed, right-skewed data, especially when small values dominate.\n",
    "   - **Yeo-Johnson Transformation:** Ideal for moderate skewness and data that includes zero or negative values.\n",
    "   - **Quantile Transformation:** Effective for severe skewness and non-Gaussian distributions, as it maps data to a normal distribution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:10.389691Z",
     "start_time": "2025-03-02T12:51:29.834575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def transform_and_store_skewed_features(df, skewed_features):\n",
    "    \"\"\"\n",
    "    Transforms skewed features and stores results without modifying df inside the function.\n",
    "    Returns a list of transformed data and skewness values.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    transformed_data = []  # Store transformation details for later plotting\n",
    "\n",
    "    for feature in skewed_features:\n",
    "        skew_before = df[feature].skew()\n",
    "\n",
    "        # Get the best transformation\n",
    "        transformed, method, orig_skew, new_skew = best_transform(df[feature])\n",
    "\n",
    "        # Store results in a list instead of modifying df directly\n",
    "        transformed_data.append({\n",
    "            \"feature\": feature,\n",
    "            \"original\": df[feature].copy(),  # Store original data before transformation\n",
    "            \"transformed\": transformed,  # Store transformed data\n",
    "            \"skew_before\": skew_before,\n",
    "            \"skew_after\": new_skew,\n",
    "            \"method\": method\n",
    "        })\n",
    "\n",
    "        results.append([feature, orig_skew, new_skew, method])\n",
    "\n",
    "        # Apply transformation to df (only if needed immediately)\n",
    "        df[feature] = transformed\n",
    "\n",
    "    # Convert results into a DataFrame\n",
    "    results_df = pd.DataFrame(results, columns=[\"Feature\", \"Skewness Before\", \"Skewness After\", \"Method\"])\n",
    "\n",
    "    return transformed_data, results_df\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Detect skewed features\n",
    "# ---------------------------\n",
    "skew_threshold = 0.5\n",
    "numerical_skew = df[numerical_features].skew()\n",
    "skewed_features = numerical_skew[abs(numerical_skew) > skew_threshold].index.tolist()\n",
    "print(\"Skewed Features:\", skewed_features)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Transform Data and Get Results\n",
    "# ---------------------------\n",
    "transformed_data, results_df = transform_and_store_skewed_features(df, skewed_features)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Display the Table FIRST\n",
    "# ---------------------------\n",
    "# display(results_df)\n",
    "# # ---------------------------\n",
    "# # Step 3: Plot Histograms for Each Transformed Feature\n",
    "# # ---------------------------\n",
    "# for data in transformed_data:\n",
    "#     feature = data[\"feature\"]\n",
    "#     skew_before = data[\"skew_before\"]\n",
    "#     new_skew = data[\"skew_after\"]\n",
    "#\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(16, 6))  # 1 Row, 2 Columns\n",
    "#\n",
    "#     # Histogram Before Transformation\n",
    "#     sns.histplot(data[\"original\"], bins=100, kde=True, ax=axes[0], color='blue', alpha=0.6)\n",
    "#     axes[0].set_title(f'Before Transformation: {feature} (Skew: {skew_before:.2f})')\n",
    "#\n",
    "#     # Histogram After Transformation\n",
    "#     sns.histplot(data[\"transformed\"], bins=100, kde=True, ax=axes[1], color='red', alpha=0.6)\n",
    "#     axes[1].set_title(f'After Transformation: {feature} (Skew: {new_skew:.2f})')\n",
    "#\n",
    "#     plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "#     plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewed Features: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V12', 'V14', 'V16', 'V17', 'V20', 'V21', 'V23', 'V24', 'V26', 'V27', 'V28', 'Amount']\n"
     ]
    }
   ],
   "execution_count": 592
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this table, we see the impact of our **automated transformation process** on the **skewness of numerical features**. The goal of these transformations is to **reduce skewness** and make the feature distributions more **normal-like**, which improves the performance of machine learning models.\n",
    "\n",
    "#### **Observations & Improvements**\n",
    "- **Most features had a high initial skewness**, either negative or positive.\n",
    "- **The process successfully reduced skewness for all features**, making them more normally distributed.\n",
    "- **Different transformation methods were applied**:\n",
    "  - **Quantile Transformation** was the most commonly chosen method, as it is effective for severe skewness.\n",
    "  - **Yeo-Johnson Transformation** was applied for `Amount` and `V4`, as it worked better in reducing skewness for these features.\n",
    "\n",
    "#### **Why Does This Matter?**\n",
    "- **Reducing skewness** helps **stabilize variance** and improves the **effectiveness of machine learning models**.\n",
    "- **Highly skewed features** can lead to models being **biased** toward extreme values.\n",
    "- By using **appropriate transformations**, we ensure that features contribute **meaningfully** to model predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:16.059995Z",
     "start_time": "2025-03-02T12:53:10.392700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # ---------------------------\n",
    "# # Step 3: Plot Only the Last Transformed Feature with Boxplots\n",
    "# # ---------------------------\n",
    "# if transformed_data:  # Ensure there's at least one feature\n",
    "#     last_feature_data = transformed_data[-1]  # Get last transformed feature\n",
    "#\n",
    "#     feature = last_feature_data[\"feature\"]\n",
    "#     skew_before = last_feature_data[\"skew_before\"]\n",
    "#     new_skew = last_feature_data[\"skew_after\"]\n",
    "#\n",
    "#     fig, axes = plt.subplots(2, 2, figsize=(20, 12))  # 2 Rows, 2 Columns\n",
    "#\n",
    "#     # Histogram Before Transformation\n",
    "#     sns.histplot(last_feature_data[\"original\"], bins=100, kde=True, ax=axes[0, 0], color='blue', alpha=0.6)\n",
    "#     axes[0, 0].set_title(f'Before Transformation: {feature} (Skew: {skew_before:.2f})')\n",
    "#\n",
    "#     # Boxplot Before Transformation\n",
    "#     sns.boxplot(x=last_feature_data[\"original\"], ax=axes[0, 1], color='cyan')\n",
    "#     axes[0, 1].set_title(f'Boxplot Before: {feature}')\n",
    "#\n",
    "#     # Histogram After Transformation\n",
    "#     sns.histplot(last_feature_data[\"transformed\"], bins=100, kde=True, ax=axes[1, 0], color='red', alpha=0.6)\n",
    "#     axes[1, 0].set_title(f'After Transformation: {feature} (Skew: {new_skew:.2f})')\n",
    "#\n",
    "#     # Boxplot After Transformation\n",
    "#     sns.boxplot(x=last_feature_data[\"transformed\"], ax=axes[1, 1], color='orange')\n",
    "#     axes[1, 1].set_title(f'Boxplot After: {feature}')\n",
    "#\n",
    "#     plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "#     plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 593
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 Conclusions from the Transformation Analysis\n",
    "**Before Transformation (Left Graph):**\n",
    "\n",
    "The feature \"Amount\" had a high skewness (16.98), meaning the distribution was heavily skewed to the right.\n",
    "A large concentration of values on the left side, with extreme outliers on the right.\n",
    "\n",
    "**Boxplot (Middle Graph):**\n",
    "\n",
    "Most values are clustered near the lower end of the range.\n",
    "A significant number of outliers are visible, indicating extreme variations in the data.\n",
    "\n",
    "**After Transformation (Right Graph):**\n",
    "\n",
    "Skewness significantly reduced (0.32), bringing the distribution closer to normal.\n",
    "The data is now more evenly spread, with reduced impact from extreme values.\n",
    "\n",
    "### 1.3 Key Takeaways\n",
    "   - The transformation successfully reduced skewness, making the data more suitable for machine learning models.\n",
    "   - The feature now has a smoother distribution, which can improve model performance.\n",
    "   - Although the transformation did not fully normalize the data, it represents a significant improvement over the original distribution."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Step 2: Addressing Class Imbalance in Machine Learning**  \n",
    "\n",
    "Class imbalance is a common challenge in datasets where one class is significantly underrepresented compared to another. This issue is especially critical in **fraud detection, medical diagnostics, and rare event classification**, where models tend to favor the majority class, leading to poor detection of minority cases.\n",
    "\n",
    "To tackle this, we implement a **balanced approach** that enhances model performance while preserving real-world data characteristics:\n",
    "\n",
    "âœ” **TF-IDF Transformation:** Converts textual data into numerical features, ensuring text-based patterns contribute effectively to classification.  \n",
    "\n",
    "âœ” **Moderate SMOTE Oversampling:** Expands the minority class to **10% of the majority class size**, improving its representation without overfitting.  \n",
    "\n",
    "âœ” **Strategic Undersampling:** Reduces the majority class to **twice the size of the minority class**, but only in the **training set**, as recommended by research for optimal generalization.  \n",
    "\n",
    "âœ” **Maintaining Real-World Distribution in the Test Set:** The test set remains **unaltered** to reflect actual class proportions, ensuring the model is evaluated under realistic conditions.  \n",
    "\n",
    "By combining **oversampling, undersampling, and TF-IDF transformation**, we create a **well-balanced training dataset** that allows the model to learn from **minority cases** without introducing bias towards artificially inflated data. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 Apply TF-IDF for Text Processing\n",
    "**Term Frequency-Inverse Document Frequency (TF-IDF)** is a statistical measure employed to evaluate the importance of a word within a document relative to a collection of documents (corpus). It is widely used in natural language processing (NLP) and information retrieval to transform textual data into numerical representations, facilitating the analysis and modeling of text by machine learning algorithms.\n",
    "\n",
    "**Components of TF-IDF:**\n",
    "\n",
    "1. **Term Frequency (TF):** This component quantifies the frequency of a term in a specific document. It reflects how often a word appears in a document, indicating its relevance within that particular context.\n",
    "\n",
    "2. **Inverse Document Frequency (IDF):** This component assesses the significance of a term across the entire corpus. It diminishes the weight of terms that occur frequently across many documents and increases the weight of terms that are rare, thereby highlighting words that are more informative and less generic.\n",
    "\n",
    "**Calculation of TF-IDF:**\n",
    "\n",
    "The TF-IDF value for a term in a document is calculated by multiplying its TF and IDF values:\n",
    "\n",
    "**_TF-IDF(t, d, D) = TF(t, d) x IDF(t, D)_** \n",
    "\n",
    "Where:\n",
    "- t represents the term;\n",
    "- d denotes the specific document;\n",
    "- D signifies the entire corpus of documents.\n",
    "\n",
    "**Applications of TF-IDF:**\n",
    "\n",
    "TF-IDF is instrumental in various NLP tasks, including:\n",
    "\n",
    "- **Text Classification:** By converting text into numerical features, TF-IDF enables machine learning models to classify documents based on their content.\n",
    "\n",
    "- **Information Retrieval:** Search engines utilize TF-IDF to rank documents by relevance to a user's query, ensuring more pertinent results are presented.\n",
    "\n",
    "- **Clustering and Topic Modeling:** TF-IDF aids in grouping similar documents and identifying underlying themes by emphasizing significant terms within the text.\n",
    "\n",
    "In the context of class imbalance, particularly in domains like fraud detection or rare event classification, TF-IDF plays a crucial role. By assigning appropriate weights to terms, it ensures that minority class characteristics are adequately represented, thereby enhancing the model's ability to detect and learn from underrepresented instances.îˆ†\n",
    "\n",
    "#### References to the Article\n",
    "**\"Data oversampling and imbalanced datasets: an investigation of performance for machine learning and feature engineering\"**\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:16.070052Z",
     "start_time": "2025-03-02T12:53:16.062006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def apply_tfidf(data, text_column):\n",
    "    \"\"\"\n",
    "    Applies TF-IDF transformation to a given text column in a DataFrame.\n",
    "    Replaces the original text column with the extracted TF-IDF features.\n",
    "\n",
    "    :param data: DataFrame containing the dataset\n",
    "    :param text_column: Name of the column containing text data\n",
    "    :return: Updated DataFrame with TF-IDF features\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ”¹ Applying TF-IDF on column: **{text_column}**\")\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(data[text_column])\n",
    "\n",
    "    # Convert sparse matrix to DataFrame\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(),\n",
    "                            columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    print(f\"Extracted {len(tfidf_df.columns)} TF-IDF features for column: {text_column}\")\n",
    "\n",
    "    # Drop the original text column and concatenate TF-IDF features\n",
    "    data = data.drop(columns=[text_column]).reset_index(drop=True)\n",
    "    data = pd.concat([data, tfidf_df], axis=1)\n",
    "\n",
    "    print(f\"New dataset shape after TF-IDF transformation: {data.shape}\")\n",
    "\n",
    "    # Show a preview of the new TF-IDF features\n",
    "    print(\"\\nPreview of TF-IDF features:\")\n",
    "    print(tfidf_df.head(3))\n",
    "\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": 594
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:16.107397Z",
     "start_time": "2025-03-02T12:53:16.072371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Detect all text columns dynamically\n",
    "text_columns = df.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "\n",
    "if text_columns:\n",
    "    print(f\"\\nðŸ“Š Detected text columns for TF-IDF transformation: {text_columns}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No text columns detected. Skipping TF-IDF transformation.\")\n",
    "\n",
    "# Apply TF-IDF transformation if text columns exist\n",
    "for col in text_columns:\n",
    "    df = apply_tfidf(df, col)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… No text columns detected. Skipping TF-IDF transformation.\n"
     ]
    }
   ],
   "execution_count": 596
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**What Happens Here?**\n",
    "\n",
    "Automatically finds text columns â†’ No need to manually specify them.\n",
    "\n",
    "Applies TF-IDF transformation â†’ Converts words into numerical values.\n",
    "\n",
    "Ensures compatibility with SMOTE â†’ Prevents errors when handling imbalanced data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Balancing The dataset initially had a highly imbalanced class distribution:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:16.187094Z",
     "start_time": "2025-03-02T12:53:16.109415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature and target split\n",
    "X = df.drop(columns=[target_column])  # Features\n",
    "y = df[target_column]  # Target variable\n",
    "\n",
    "# Display class distribution before balancing\n",
    "print(\"Class Distribution Before Balancing:\", Counter(y))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Class Distribution Before Balancing:\n",
    "\n",
    "    - Class 0 (No Fraud): 284,315 samples\n",
    "    - Class 1 (Fraud): Only 492 samples!\n",
    "This is extremely imbalanced!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.1. Applying SMOTE and its Variants for Imbalanced Data\n",
    "#### Data oversampling and imbalanced datasets: an investigation of performance for machine learning and feature engineering Muhammad Mujahid et al. (2024)\n",
    "#### Overview\n",
    "Class imbalance is a critical issue in machine learning that can lead to biased models favoring the majority class. Oversampling techniques, particularly **Synthetic Minority Oversampling Technique (SMOTE)** and its variants, help address this issue by generating synthetic samples for the minority class. This notebook explores the application of **SMOTE, Borderline-SMOTE, SVM-SMOTE, and ADASYN** for handling imbalanced datasets.\n",
    "\n",
    "#### Why SMOTE?\n",
    "According to the article **\"Data oversampling and imbalanced datasets: an investigation of performance for machine learning and feature engineering\"** by Mujahid et al. (2024), oversampling techniques significantly improve model performance when dealing with class-imbalanced datasets. The study found that:\n",
    "\n",
    "- **SMOTE and ADASYN achieved higher accuracy** compared to other oversampling techniques.\n",
    "- **Support Vector Machine (SVM) with a linear kernel** achieved the highest accuracy (99.67%) when using **ADASYN oversampled datasets with TF-IDF features**.\n",
    "- **Borderline-SMOTE improved classification along decision boundaries**, preventing models from learning misleading patterns from majority classes.\n",
    "- **SVM-SMOTE uses support vectors to generate synthetic samples**, making it effective for datasets where decision boundaries are complex.\n",
    "\n",
    "#### References to the Article\n",
    "- **Synthetic Minority Oversampling Technique (SMOTE):** Introduced by Chawla et al. and used to generate synthetic samples by interpolating between existing minority class samples. ([Section: Description of oversampling techniques])\n",
    "- **Borderline-SMOTE:** Enhances SMOTE by focusing on samples near the decision boundary, preventing overfitting. ([Section: Border-line SMOTE])\n",
    "- **SVM-SMOTE:** Uses support vectors instead of KNN to generate synthetic samples, making it useful for complex boundaries. ([Section: SVM-SMOTE])\n",
    "- **ADASYN:** Generates synthetic data adaptively based on data density, improving learning for underrepresented minority class samples. ([Section: Adaptive synthetic oversampling])\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.2. SMOTE Implementation in Python\n",
    "The function below applies different SMOTE variations to balance an imbalanced dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:17.510678Z",
     "start_time": "2025-03-02T12:53:16.189099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def apply_smote(X, y, smote_type='SMOTE', sampling_strategy=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply different SMOTE variations to balance an imbalanced dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature matrix\n",
    "    - y: Target labels\n",
    "    - smote_type: Type of SMOTE ('SMOTE', 'Borderline', 'SVM', 'ADASYN')\n",
    "    - sampling_strategy: Ratio of minority to majority class\n",
    "    - random_state: Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - X_resampled: Resampled feature matrix\n",
    "    - y_resampled: Resampled target labels\n",
    "    \"\"\"\n",
    "    smote_mapping = {\n",
    "        'SMOTE': SMOTE(random_state=random_state),\n",
    "        'Borderline': BorderlineSMOTE(random_state=random_state),\n",
    "        'SVM': SVMSMOTE(random_state=random_state),\n",
    "        'ADASYN': ADASYN(random_state=random_state),\n",
    "    }\n",
    "\n",
    "    if smote_type not in smote_mapping:\n",
    "        raise ValueError(\"Invalid smote_type. Choose from 'SMOTE', 'Borderline', 'SVM', 'ADASYN'.\")\n",
    "\n",
    "    smote = smote_mapping[smote_type]\n",
    "    class_counts = Counter(y)\n",
    "    num_classes = len(class_counts)\n",
    "\n",
    "    print(f\"\\nClass Distribution Before {smote_type}: {class_counts}\")\n",
    "\n",
    "    # If binary classification, apply the original behavior\n",
    "    if num_classes == 2:\n",
    "        smote.set_params(sampling_strategy=sampling_strategy)\n",
    "\n",
    "    # If multi-class, adjust sampling strategy\n",
    "    else:\n",
    "        print(f\"\\nðŸ”¹ Multi-class detected ({num_classes} classes). Adjusting strategy.\")\n",
    "        majority_class = max(class_counts, key=class_counts.get)\n",
    "\n",
    "        # Create a dictionary for multi-class balancing\n",
    "        sampling_strategy_dict = {\n",
    "            class_label: max(int(class_counts[majority_class] * sampling_strategy), count)\n",
    "            for class_label, count in class_counts.items() if count > 5 and class_label != majority_class\n",
    "        }\n",
    "\n",
    "        if sampling_strategy_dict:\n",
    "            smote.set_params(sampling_strategy=sampling_strategy_dict)\n",
    "            print(f\"ðŸ”¹ **Adjusted Sampling Strategy:** {sampling_strategy_dict}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Not enough samples for ADASYN. Switching to SMOTE instead.\")\n",
    "            smote = SMOTE(sampling_strategy='auto', random_state=random_state)\n",
    "\n",
    "    # Apply SMOTE/ADASYN and catch errors\n",
    "    try:\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nâš ï¸ {smote_type} Error: {e}\")\n",
    "        print(\"ðŸ‘‰ Switching to standard SMOTE instead.\")\n",
    "        smote = SMOTE(sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    class_counts_after = Counter(y_resampled)\n",
    "    print(f\"\\nClass Distribution After {smote_type}: {class_counts_after}\")\n",
    "\n",
    "    # Visualization of class distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()), ax=axes[0])\n",
    "    axes[0].set_title(f\"Before {smote_type}\")\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "    sns.barplot(x=list(class_counts_after.keys()), y=list(class_counts_after.values()), ax=axes[1])\n",
    "    axes[1].set_title(f\"After {smote_type}\")\n",
    "    axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "X_smote, y_smote = apply_smote(X, y, smote_type='ADASYN')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Explanation of the Process\n",
    "- The function **supports different SMOTE variations**: `SMOTE`, `Borderline-SMOTE`, `SVM-SMOTE`, and `ADASYN`.\n",
    "- It prints **class distribution before and after oversampling**.\n",
    "- **Visualizes the class distribution** to show the effect of oversampling.\n",
    "\n",
    "#### Observations Based on the Article\n",
    "- **SMOTE and ADASYN provide the best performance** on imbalanced Twitter datasets.\n",
    "- **SVM with linear kernel showed the highest accuracy (99.67%) on ADASYN-generated data**.\n",
    "- **TF-IDF features performed better** than BoW for text classification.\n",
    "- **Borderline-SMOTE is useful when minority samples are close to decision boundaries**.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "- **Why Use This?** The choice of oversampling strategy can impact model performance, as seen in the study. Selecting **ADASYN or SMOTE** typically leads to **higher accuracy**.\n",
    "- **Recommended Approach:** If working with text data, use **TF-IDF** for feature extraction before applying SMOTE.\n",
    "- **Best Performing Model:** **SVM with ADASYN** oversampling achieved **99.67% accuracy**, making it the most reliable choice."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Class Distribution After SMOTE Balancing:\n",
    "\n",
    "    - Class 0 (No Fraud): 284,315 samples\n",
    "    - Class 1 (Fraud): 28526 synthetic samples"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.1.2. Apply undersampling\n",
    " Majority class will be twice the minority class size"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:17.655849Z",
     "start_time": "2025-03-02T12:53:17.512725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "def apply_undersampling(X, y, sampling_ratio=0.5, random_state=42, max_classes_display=10):\n",
    "    \"\"\"\n",
    "    Apply Random Undersampling to balance a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature matrix\n",
    "    - y: Target labels\n",
    "    - sampling_ratio: Ratio of minority to majority class for binary;\n",
    "                      for multi-class, a dictionary will be generated.\n",
    "    - random_state: Random seed for reproducibility.\n",
    "    - max_classes_display: Maximum number of classes to display in the output.\n",
    "\n",
    "    Returns:\n",
    "    - X_balanced: Resampled feature matrix.\n",
    "    - y_balanced: Resampled target labels.\n",
    "    \"\"\"\n",
    "    class_counts = Counter(y)\n",
    "    num_classes = len(class_counts)\n",
    "\n",
    "    # ðŸ“Š **Print Class Distribution Before**\n",
    "    print(\"\\nðŸ“Š **Class Distribution Before Undersampling:**\")\n",
    "\n",
    "    if num_classes <= 2:  # Binary classification - print full details\n",
    "        for label, count in class_counts.items():\n",
    "            print(f\"- Class {label}: {count} samples\")\n",
    "    else:  # Multi-class - Show only the most frequent classes\n",
    "        print(f\"ðŸ”¹ Multi-class detected ({num_classes} unique classes). Showing top {max_classes_display}:\")\n",
    "        most_common_classes = class_counts.most_common(max_classes_display)\n",
    "        for label, count in most_common_classes:\n",
    "            print(f\"- Class {label}: {count} samples\")\n",
    "        print(f\"... ({num_classes - max_classes_display} more classes not shown)\")\n",
    "\n",
    "    # Multi-class: Adjust strategy\n",
    "    if num_classes > 2:\n",
    "        min_class_size = min(class_counts.values())  # Find the smallest class size\n",
    "        if min_class_size <= 1:\n",
    "            print(\"\\nâš ï¸ Warning: Some classes have only 1 sample. Adjusting undersampling strategy.\")\n",
    "            sampling_strategy_dict = {\n",
    "                class_label: min(class_counts[class_label], max(1, int(class_counts[class_label] * sampling_ratio)))\n",
    "                for class_label in class_counts\n",
    "            }\n",
    "        else:\n",
    "            sampling_strategy_dict = {\n",
    "                class_label: max(1, int(class_counts[class_label] * sampling_ratio))\n",
    "                for class_label in class_counts\n",
    "            }\n",
    "\n",
    "        undersample = RandomUnderSampler(sampling_strategy=sampling_strategy_dict, random_state=random_state)\n",
    "\n",
    "    else:  # Binary classification\n",
    "        undersample = RandomUnderSampler(sampling_strategy=sampling_ratio, random_state=random_state)\n",
    "\n",
    "    try:\n",
    "        X_balanced, y_balanced = undersample.fit_resample(X, y)\n",
    "        class_counts_after = Counter(y_balanced)\n",
    "\n",
    "        # ðŸ“Š **Print Class Distribution After**\n",
    "        print(\"\\nâœ… **Class Distribution After Undersampling:**\")\n",
    "        if num_classes <= 2:  # Binary case\n",
    "            for label, count in class_counts_after.items():\n",
    "                print(f\"- Class {label}: {count} samples\")\n",
    "        else:  # Multi-class case, show summary\n",
    "            print(f\"ðŸ”¹ {num_classes} classes after undersampling. Showing top {max_classes_display}:\")\n",
    "            most_common_after = class_counts_after.most_common(max_classes_display)\n",
    "            for label, count in most_common_after:\n",
    "                print(f\"- Class {label}: {count} samples\")\n",
    "            print(f\"... ({num_classes - max_classes_display} more classes not shown)\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nâš ï¸ Error during undersampling: {e}\")\n",
    "        print(\"ðŸ‘‰ Falling back to equal class distribution.\")\n",
    "        undersample = RandomUnderSampler(sampling_strategy=\"auto\", random_state=random_state)\n",
    "        X_balanced, y_balanced = undersample.fit_resample(X, y)\n",
    "        print(f\"âœ… Successfully applied equal class distribution.\")\n",
    "\n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "\n",
    "\n",
    "# Apply undersampling to the dataset\n",
    "\n",
    "X_balanced, y_balanced = apply_undersampling(X_smote, y_smote)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "If left unbalanced, a model would likely predict \"No Fraud\" for almost everything and achieve high accuracy but poor fraud detection.\n",
    "\n",
    "The model would struggle to generalize patterns from only 492 fraud cases.\n",
    "\n",
    "Class Distribution After Undersampling:\n",
    "\n",
    "    - Class 0 (No Fraud): 57052 samples\n",
    "    - Class 1 (Fraud): 28526 synthetic samples\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Visualizing Class Distribution Before and After Balancing**"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:18.867460Z",
     "start_time": "2025-03-02T12:53:17.657859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot class distribution before and after balancing\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.countplot(x=y, ax=axes[0])\n",
    "axes[0].set_title(\"Class Distribution Before Balancing\")\n",
    "axes[0].set_xlabel(\"Class\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "sns.countplot(x=y_balanced, ax=axes[1])\n",
    "axes[1].set_title(\"Class Distribution After SMOTE + Undersampling\")\n",
    "axes[1].set_xlabel(\"Class\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:20.189438Z",
     "start_time": "2025-03-02T12:53:18.869478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_evaluate_lgbm(X, y, X_balanced, y_balanced, test_size=0.2, train_size=0.3, random_state=42):\n",
    "    # Split the imbalanced dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, train_size=train_size, random_state=random_state, stratify=y)\n",
    "\n",
    "    # Split the balanced dataset\n",
    "    X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(\n",
    "        X_balanced, y_balanced, test_size=test_size, train_size=train_size, random_state=random_state, stratify=y_balanced\n",
    "    )\n",
    "\n",
    "    # Define LightGBM classifier parameters\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'random_state': random_state\n",
    "    }\n",
    "\n",
    "    # Train LightGBM on imbalanced dataset\n",
    "    start_time = time.time()\n",
    "    model_lgb = lgb.LGBMClassifier(**lgb_params)\n",
    "    model_lgb.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Predictions on imbalanced test data\n",
    "    y_pred_lgb = model_lgb.predict(X_test)\n",
    "    y_probs_lgb = model_lgb.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "    # Evaluate performance on imbalanced dataset\n",
    "    print(f\"Performance on Imbalanced Dataset (Training Time: {end_time - start_time:.2f} sec):\")\n",
    "    print(classification_report(y_test, y_pred_lgb))\n",
    "\n",
    "    # Train LightGBM on balanced dataset\n",
    "    start_time = time.time()\n",
    "    model_lgb_bal = lgb.LGBMClassifier(**lgb_params)\n",
    "    model_lgb_bal.fit(X_train_bal, y_train_bal)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Predictions on balanced test data\n",
    "    y_pred_bal_lgb = model_lgb_bal.predict(X_test_bal)\n",
    "    y_probs_bal_lgb = model_lgb_bal.predict_proba(X_test_bal)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "    # Evaluate performance on balanced dataset\n",
    "    print(f\"Performance on Balanced Dataset (Training Time: {end_time - start_time:.2f} sec):\")\n",
    "    print(classification_report(y_test_bal, y_pred_bal_lgb))\n",
    "\n",
    "    return y_test, y_pred_lgb, y_probs_lgb, y_test_bal, y_pred_bal_lgb, y_probs_bal_lgb\n",
    "\n",
    "# Example call to the function\n",
    "y_test, y_pred_lgb, y_probs_lgb, y_test_bal, y_pred_bal_lgb, y_probs_bal_lgb = train_evaluate_lgbm(X, y, X_balanced, y_balanced)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Explanation of LightGBM Results on Imbalanced vs. Balanced Datasets**\n",
    "\n",
    "#### **1. Understanding the Context**\n",
    "The output shows the results of **LightGBM (a gradient boosting model)** trained on both an **imbalanced dataset** and a **balanced dataset**. The dataset appears to be a **binary classification task** (fraud detection or another imbalanced scenario) with labels:\n",
    "- **Class 0** (Negative class, majority)\n",
    "- **Class 1** (Positive class, minority)\n",
    "\n",
    "The model's performance is measured using **precision, recall, f1-score, and accuracy** on each dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## **Imbalanced Dataset Training Results**\n",
    "```\n",
    "[LightGBM] [Info] Number of positive: 148, number of negative: 85294\n",
    "```\n",
    "- **148 fraud cases (positive class)**\n",
    "- **85,294 non-fraud cases (negative class)**\n",
    "- **Highly imbalanced (~0.17% fraud cases)**\n",
    "\n",
    "### **Training Info**\n",
    "```\n",
    "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007051 seconds.\n",
    "[LightGBM] [Info] Total Bins 7650\n",
    "[LightGBM] [Info] Number of data points in the train set: 85442, number of used features: 30\n",
    "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001732 -> initscore=-6.356647\n",
    "[LightGBM] [Info] Start training from score -6.356647\n",
    "```\n",
    "- **Training on 85,442 samples** with **30 features**\n",
    "- The prior probability of fraud cases (`pavg=0.001732`) is extremely low.\n",
    "- The **initial model score** for LightGBM is **-6.356647**, meaning it's starting heavily biased toward the majority class.\n",
    "\n",
    "### **Performance on Imbalanced Dataset**\n",
    "```\n",
    "Performance on Imbalanced Dataset (Training Time: 0.49 sec):\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00     56864\n",
    "           1       0.50      0.65      0.57        98\n",
    "\n",
    "    accuracy                           1.00     56962\n",
    "   macro avg       0.75      0.83      0.78     56962\n",
    "weighted avg       1.00      1.00      1.00     56962\n",
    "```\n",
    "\n",
    "### **Key Observations**\n",
    "- **Accuracy = 1.00 (~100%)** â†’ **Misleading due to class imbalance**\n",
    "- **Class 0 (Majority, non-fraud)**: Precision, recall, and F1-score are all **1.00** (perfect) because almost all predictions are for Class 0.\n",
    "- **Class 1 (Minority, fraud cases)**:\n",
    "  - **Precision = 0.50** â†’ Only 50% of predicted fraud cases were actually fraud.\n",
    "  - **Recall = 0.65** â†’ The model correctly detected 65% of actual fraud cases but still missed 35%.\n",
    "  - **F1-score = 0.57** â†’ Imbalanced performance; the model struggles with fraud detection.\n",
    "\n",
    "#### **Problem with Imbalanced Data**\n",
    "- The model is highly **biased toward the majority class**.\n",
    "- Fraud cases are often **undetected or misclassified**.\n",
    "- **Precision (0.50) is low**, meaning there are a lot of false positives.\n",
    "- **Recall (0.65) is also not great**, meaning some fraud cases are missed.\n",
    "\n",
    "---\n",
    "\n",
    "## **Balanced Dataset Training Results**\n",
    "```\n",
    "[LightGBM] [Info] Number of positive: 8558, number of negative: 17115\n",
    "```\n",
    "- **8,558 fraud cases**\n",
    "- **17,115 non-fraud cases**\n",
    "- **Balanced dataset (1:2 ratio of fraud to non-fraud cases)**\n",
    "\n",
    "### **Training Info**\n",
    "```\n",
    "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002783 seconds.\n",
    "[LightGBM] [Info] Total Bins 7650\n",
    "[LightGBM] [Info] Number of data points in the train set: 25673, number of used features: 30\n",
    "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.333346 -> initscore=-0.693089\n",
    "[LightGBM] [Info] Start training from score -0.693089\n",
    "```\n",
    "- **Training on 25,673 samples** (less than the imbalanced dataset, due to balancing techniques).\n",
    "- The prior probability of fraud cases (`pavg=0.333346`) is **significantly higher** than in the imbalanced dataset.\n",
    "- **Initial model score (-0.693089)** is closer to a fair classification (compared to -6.356647 before).\n",
    "\n",
    "### **Performance on Balanced Dataset**\n",
    "```\n",
    "Performance on Balanced Dataset (Training Time: 0.26 sec):\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      1.00      1.00     11411\n",
    "           1       1.00      0.99      0.99      5705\n",
    "\n",
    "    accuracy                           0.99     17116\n",
    "   macro avg       0.99      0.99      0.99     17116\n",
    "weighted avg       0.99      0.99      0.99     17116\n",
    "```\n",
    "\n",
    "### **Key Observations**\n",
    "- **Accuracy = 0.99 (~99%)** â†’ Still high, but more reliable now.\n",
    "- **Class 0 (Non-fraud cases)**:\n",
    "  - **Precision = 0.99, Recall = 1.00, F1-score = 1.00** â†’ Almost perfect.\n",
    "- **Class 1 (Fraud cases)**:\n",
    "  - **Precision = 1.00** â†’ Every fraud case predicted was actually fraud.\n",
    "  - **Recall = 0.99** â†’ Almost every actual fraud case was detected.\n",
    "  - **F1-score = 0.99** â†’ Balanced model performance.\n",
    "\n",
    "#### **Why Balanced Data Performs Better**\n",
    "- The **recall for fraud cases increased from 0.65 to 0.99** â†’ The model is now detecting **almost all** fraud cases.\n",
    "- The **precision for fraud cases increased from 0.50 to 1.00** â†’ Fewer false positives.\n",
    "- The **model generalizes better** because it learns from an equal distribution of both classes.\n",
    "- **False negative cases (missed fraud cases) are dramatically reduced**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Comparison: Imbalanced vs. Balanced**\n",
    "| Metric        | Imbalanced Dataset | Balanced Dataset |\n",
    "|--------------|------------------|----------------|\n",
    "| **Accuracy** | **1.00** (Misleading) | **0.99** (More reliable) |\n",
    "| **Class 1 Precision (Fraud)** | **0.50** (50% of predicted fraud cases are correct) | **1.00** (No false positives) |\n",
    "| **Class 1 Recall (Fraud)** | **0.65** (Misses 35% of fraud cases) | **0.99** (Almost all fraud cases detected) |\n",
    "| **F1-score (Fraud)** | **0.57** | **0.99** |\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "- The **imbalanced dataset caused poor fraud detection**, with **low recall (0.65) and precision (0.50)**.\n",
    "- The **balanced dataset (via SMOTE or undersampling) improved model performance dramatically**.\n",
    "- **Handling class imbalance is crucial for fraud detection models**; otherwise, models will be **biased toward the majority class**.\n",
    "- **Recommendation:** Always **balance datasets** in classification problems where the minority class is underrepresented (e.g., fraud detection, rare disease classification)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Confusion Matrices for Model Evaluation**"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:20.568820Z",
     "start_time": "2025-03-02T12:53:20.190445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Fraud\", \"Fraud\"], yticklabels=[\"No Fraud\", \"Fraud\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Confusion matrix for the imbalanced model\n",
    "plot_confusion_matrix(y_test, y_pred_lgb, \"Confusion Matrix - Imbalanced Dataset\")\n",
    "\n",
    "# Confusion matrix for the balanced model\n",
    "plot_confusion_matrix(y_test_bal, y_pred_bal_lgb, \"Confusion Matrix - Balanced Dataset\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Confusion Matrix for the Imbalanced Dataset\n",
    "\n",
    "| **Actual / Predicted** | **No Fraud (0)** | **Fraud (1)** |\n",
    "|------------------------|-----------------|---------------|\n",
    "| **No Fraud (0)**       | **56800** (TN)  | **64** (FP)  |\n",
    "| **Fraud (1)**          | **34** (FN)     | **64** (TP)  |\n",
    "\n",
    "- **TN (True Negative: 56800)** â†’ Model **correctly** predicted non-fraud cases.\n",
    "- **FP (False Positive: 64)** â†’ Model **incorrectly** predicted fraud when it was actually non-fraud.\n",
    "- **FN (False Negative: 34)** â†’ Model **missed 34 fraud cases**, predicting them as non-fraud.\n",
    "- **TP (True Positive: 64)** â†’ Model **correctly** identified 64 fraud cases.\n",
    "\n",
    "#### Observations*\n",
    "**High Accuracy**: The model performs well overall because most transactions are **non-fraud**.\n",
    "**Poor Fraud Detection**: Only **64 fraud cases** detected, missing **34 fraud transactions**.\n",
    "**High Class Imbalance Impact**: The model is biased toward non-fraud cases due to the imbalanced dataset.\n",
    "\n",
    "\n",
    "\n",
    "### Confusion Matrix for the Balanced Dataset\n",
    "\n",
    "| **Actual / Predicted** | **No Fraud (0)** | **Fraud (1)** |\n",
    "|------------------------|------------------|---------------|\n",
    "| **No Fraud (0)**       | **11389** (TN)   | **22** (FP)   |\n",
    "| **Fraud (1)**          | **83** (FN)       | **5622** (TP)   |\n",
    "\n",
    "- **TN (True Negative: 11389)** â†’ Model **correctly** predicted non-fraud cases.\n",
    "- **FP (False Positive: 22)** â†’ Model **incorrectly** predicted fraud when it was actually non-fraud.\n",
    "- **FN (False Negative: 83)** â†’ Model **missed 83 fraud cases**, predicting them as non-fraud.\n",
    "- **TP (True Positive: 5622)** â†’ Model **correctly** identified 5622 fraud cases.\n",
    "\n",
    "#### Improvements After Balancing\n",
    "**Much Better Fraud Detection**: **5622 fraud cases** correctly detected, compared to only **64** before.\n",
    "**Lower False Positives**: The model only wrongly flagged **16** transactions as fraud.\n",
    "**Increased Sensitivity to Fraud**: The model now **detects more fraud cases** at the cost of a few extra false positives.\n",
    "\n",
    "### Key Takeaways\n",
    "- The **imbalanced dataset** model **performs well in overall accuracy**, but **fails to detect fraud effectively**.\n",
    "- The **balanced dataset** model **detects fraud cases much better**, reducing **false negatives** (missed fraud cases).\n",
    "- **Balancing the dataset (e.g., SMOTE, undersampling)** **improves fraud detection**, making the model **more reliable**.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:20.850263Z",
     "start_time": "2025-03-02T12:53:20.570826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_precision_recall(y_true, y_probs, title):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Compare Precision-Recall Curves using returned probability predictions\n",
    "plot_precision_recall(y_test, y_probs_lgb, \"Precision-Recall Curve - Imbalanced Dataset\")\n",
    "plot_precision_recall(y_test_bal, y_probs_bal_lgb, \"Precision-Recall Curve - Balanced Dataset\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Explanation of Precision-Recall (PR) Curves**\n",
    "The **Precision-Recall (PR) Curve** helps evaluate the model's ability to **correctly detect fraud (positive class) while minimizing false positives**. It is particularly useful for **imbalanced datasets**, like our case of fraud detection.\n",
    "\n",
    "The two graphs compare the **imbalanced dataset** (before applying SMOTE/undersampling) and the **balanced dataset** (after applying resampling techniques).\n",
    "\n",
    "---\n",
    "\n",
    "## **Precision-Recall Curve for the Imbalanced Dataset**\n",
    "- The **curve starts at high precision (~1.0) but drops quickly** as recall increases.\n",
    "- This means that as the model tries to detect more fraud cases (**higher recall**), it **starts making a lot of false positives**, causing **precision to drop**.\n",
    "- The **rapid drop** suggests the model is **highly biased toward the majority class** (non-fraud), leading to **poor fraud detection**.\n",
    "- **Key Takeaway:** The model **fails to maintain good precision as recall increases**, indicating **poor fraud detection performance**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Precision-Recall Curve for the Balanced Dataset (Top Image)**\n",
    "- The curve is **much more stable**, maintaining **high precision (~1.0) across most recall values**.\n",
    "- This suggests that the **model now correctly detects fraud cases without sacrificing precision**.\n",
    "- The **almost flat precision** at **high recall values** means the model **consistently identifies fraud cases correctly** with very few false positives.\n",
    "- **Key Takeaway:** **Balancing the dataset (e.g., SMOTE or undersampling) significantly improves fraud detection** by ensuring the model does not ignore minority-class fraud cases.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "- **Imbalanced Dataset:** The model **struggles to detect fraud**. Precision drops quickly as it tries to detect more fraud cases.\n",
    "- **Balanced Dataset:** The model **performs much better**, maintaining **high precision** even at higher recall.\n",
    "- **Overall:** **Balancing the dataset helps improve fraud detection performance** by reducing false negatives (missed fraud) and keeping false positives under control."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Data Preprocessing and Model Training Pipeline\n",
    "This stage focuses on automating data preprocessing and model training in a structured and reusable way.\n",
    "The goal is to create a generalized pipeline that can be applied to any dataset with similar characteristics.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Step 1: Load the Dataset**\n",
    "Since `winequality-white.csv` is different from `creditcard.csv`, we must load it while ensuring:\n",
    "- **The target column is correctly identified** (likely `\"quality\"` instead of `\"Class\"`).\n",
    "- **All numerical features are extracted properly**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the new dataset\n",
    "target_column = 'price'\n",
    "df, numerical_features = load_data(\"USA_cars_datasets.csv\",target_column)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Step 2: Detect Skewed Features**\n",
    "The dataset contains numerical features (such as acidity, pH, alcohol, etc.). We check for **highly skewed features** using the function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Detect skewed numerical features\n",
    "skewed_features, skewness_table = detect_skewed_features(df, numerical_features)\n",
    "\n",
    "# Display the skewness table\n",
    "display(skewness_table)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Step 3: Transform Skewed Features**\n",
    "To **normalize** the dataset and reduce skewness, apply the `best_transform()` function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transform and store skewed features\n",
    "transformed_data, results_df = transform_and_store_skewed_features(df, skewed_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display transformation results\n",
    "display(results_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Step 4: Visualize the Before & After Skewness**\n",
    "Since we previously plotted histograms, you can reuse the same approach."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot transformed distributions\n",
    "for data in transformed_data:\n",
    "    feature = data[\"feature\"]\n",
    "    skew_before = data[\"skew_before\"]\n",
    "    new_skew = data[\"skew_after\"]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))  # 2 Rows, 2 Columns\n",
    "\n",
    "    # Histogram Before Transformation\n",
    "    sns.histplot(data[\"original\"], bins=100, kde=True, ax=axes[0, 0], color='blue', alpha=0.6)\n",
    "    axes[0, 0].set_title(f'Before Transformation: {feature} (Skew: {skew_before:.2f})')\n",
    "\n",
    "    # Boxplot Before Transformation\n",
    "    sns.boxplot(x=data[\"original\"], ax=axes[0, 1], color='cyan')\n",
    "    axes[0, 1].set_title(f'Boxplot Before: {feature}')\n",
    "\n",
    "    # Histogram After Transformation\n",
    "    sns.histplot(data[\"transformed\"], bins=100, kde=True, ax=axes[1, 0], color='red', alpha=0.6)\n",
    "    axes[1, 0].set_title(f'After Transformation: {feature} (Skew: {new_skew:.2f})')\n",
    "\n",
    "    # Boxplot After Transformation\n",
    "    sns.boxplot(x=data[\"transformed\"], ax=axes[1, 1], color='orange')\n",
    "    axes[1, 1].set_title(f'Boxplot After: {feature}')\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Detect all text columns dynamically\n",
    "text_columns = df.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "\n",
    "if text_columns:\n",
    "    print(f\"\\nðŸ“Š Detected text columns for TF-IDF transformation: {text_columns}\")\n",
    "    # Apply TF-IDF transformation to each text column\n",
    "    for col in text_columns:\n",
    "        df = apply_tfidf(df, col)\n",
    "else:\n",
    "    print(\"\\nâœ… No text columns detected. Skipping TF-IDF transformation.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **ðŸš€ Step 5: Apply SMOTE or Undersampling (If Needed)**\n",
    "Since **wine quality** is a **multi-class classification problem** (not binary fraud detection), SMOTE and undersampling **need adjustments**.\n",
    "\n",
    "- If **some wine quality labels are underrepresented**, apply **SMOTE**.\n",
    "- If **some labels are overrepresented**, apply **undersampling**.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract features (X) and target (y)\n",
    "X = df[numerical_features]\n",
    "y = df[target_column]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance classes\n",
    "X_smote, y_smote = apply_smote(X, y, smote_type='ADASYN')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply undersampling if necessary\n",
    "X_balanced, y_balanced = apply_undersampling(X_smote, y_smote)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### **ðŸš€ Step 6: Apply TF-IDF (If There Are Text Columns)**\n",
    "If the wine dataset contains text columns (unlikely, but possible for descriptions), you can **convert text features using TF-IDF**.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = apply_tfidf(df, text_column=\"description\")  # Replace with actual text column name\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Data Preprocessing and Model Training Pipeline\n",
    "This stage focuses on automating data preprocessing and model training in a structured and reusable way.\n",
    "The goal is to create a generalized pipeline that can be applied to any dataset with similar characteristics.\n",
    "\n",
    "Steps in This Stage:\n",
    "### 3.1 Splitting Dataset into Features and Target\n",
    "First, we separate the dataset into the feature matrix X and target vector y. This function lets you specify the name of the target column, so it works for any dataset:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:20.857226Z",
     "start_time": "2025-03-02T12:53:20.852271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_features_target(df, target_col):\n",
    "    \"\"\"\n",
    "    Split a DataFrame into feature matrix X and target vector y.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input dataset.\n",
    "        target_col (str): Name of the target column in df.\n",
    "    Returns:\n",
    "        X (pd.DataFrame): Features (all columns except target_col).\n",
    "        y (pd.Series): Target labels.\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col].copy()\n",
    "    return X, y\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Balancing Classes (Oversampling & Undersampling)\n",
    "To address class imbalance generically, we create a function that can oversample minority classes and undersample majority classes based on flexible criteria. By default, weâ€™ll use SMOTE for oversampling and random undersampling for majority classes, but the thresholds are tunable:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:22.086912Z",
     "start_time": "2025-03-02T12:53:20.859234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def balance_classes(X, y, oversample_target=0.6, undersample_factor=1.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Balance class distribution by oversampling minorities and undersampling majorities.\n",
    "    \n",
    "    Parameters:\n",
    "        X (pd.DataFrame): Feature matrix.\n",
    "        y (pd.Series or np.array): Target labels.\n",
    "        oversample_target (float): Fraction of the largest class count to oversample minorities to.\n",
    "                                   For example, 0.6 means upsample minority classes to ~60% of the max class size.\n",
    "        undersample_factor (float): Maximum ratio of majority class size to minority class size after balancing.\n",
    "                                    For example, 1.5 means no class will have more than 1.5x the number of samples of the smallest class.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "        X_bal (pd.DataFrame): Balanced feature matrix.\n",
    "        y_bal (pd.Series): Balanced target labels.\n",
    "    \"\"\"\n",
    "    # Count original class frequencies\n",
    "    class_counts = Counter(y)\n",
    "    max_class = max(class_counts, key=class_counts.get)      # class with largest count\n",
    "    max_count = class_counts[max_class]\n",
    "    \n",
    "    # Define oversampling strategy: target counts for classes below the threshold\n",
    "    oversample_strategy = {}\n",
    "    for cls, count in class_counts.items():\n",
    "        if count < oversample_target * max_count:\n",
    "            # Set the new target count for this class (e.g., 0.6 * max_count)\n",
    "            oversample_strategy[cls] = int(oversample_target * max_count)\n",
    "    # Apply SMOTE oversampling for specified classes\n",
    "    if oversample_strategy:\n",
    "        smote = SMOTE(sampling_strategy=oversample_strategy, random_state=random_state)\n",
    "        X_res, y_res = smote.fit_resample(X, y)\n",
    "    else:\n",
    "        X_res, y_res = X.copy(), y.copy()\n",
    "    \n",
    "    # After oversampling, get the smallest class count\n",
    "    new_counts = Counter(y_res)\n",
    "    min_count = min(new_counts.values())\n",
    "    \n",
    "    # Define undersampling strategy: limit classes that are more than undersample_factor * min_count\n",
    "    undersample_strategy = {}\n",
    "    for cls, count in new_counts.items():\n",
    "        limit = int(undersample_factor * min_count)\n",
    "        if count > limit:\n",
    "            undersample_strategy[cls] = limit  # cap this class to the limit\n",
    "    # Apply undersampling for specified classes\n",
    "    if undersample_strategy:\n",
    "        rus = RandomUnderSampler(sampling_strategy=undersample_strategy, random_state=random_state)\n",
    "        X_bal, y_bal = rus.fit_resample(X_res, y_res)\n",
    "    else:\n",
    "        X_bal, y_bal = X_res, y_res\n",
    "    \n",
    "    return X_bal, y_bal\n",
    "\n",
    "# Example usage on an imbalanced dataset:\n",
    "X_bal, y_bal = balance_classes(X, y, oversample_target=0.6, undersample_factor=1.5)\n",
    "print(\"Class counts after balancing:\", Counter(y_bal))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "How this works:\n",
    "- We oversample any class that has fewer than oversample_target * (size of largest class). For instance, if the largest class has 1000 samples and oversample_target=0.6, then any class with <600 samples will be increased (via SMOTE) up to roughly 600. This parameter is flexible â€“ setting it to 1.0 would fully balance to the majority class size, while a lower value avoids excessive synthetic data.\n",
    "- After oversampling, we undersample any class that now has more than undersample_factor * (size of smallest class). For example, with undersample_factor=1.5, if the smallest class has 600 samples, no other class will be allowed more than 900; the majority class (if larger) is randomly down-sampled to 900. This prevents any class from still dominating the dataset.\n",
    "- Both oversample_target and undersample_factor can be tuned per dataset. For datasets with extreme imbalance, you might oversample to 100% and undersample to equal counts; for milder cases, smaller adjustments may suffice.\n",
    "- We use SMOTE and RandomUnderSampler from imblearn, but you could plug in different oversampling or undersampling methods if needed (the function could accept a different sampler object via parameters).\n",
    "\n",
    "The result is a new balanced dataset (X_bal, y_bal) with a much more even class distribution, regardless of the original datasetâ€™s class labels or imbalance severity."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3 Correcting Skewness in Numeric Features\n",
    "This function will automatically try a few transformations and pick the one that best normalizes the data:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:22.568476Z",
     "start_time": "2025-03-02T12:53:22.089920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def reduce_skewness(feature_series, methods=('boxcox', 'yeo-johnson', 'log'), plot=False):\n",
    "    \"\"\"\n",
    "    Transform a numeric feature to reduce skewness.\n",
    "    \n",
    "    Tries multiple transformations and chooses the one with lowest absolute skew.\n",
    "    Parameters:\n",
    "        feature_series (pd.Series or np.array): The data for a single feature.\n",
    "        methods (tuple of str): Transformation methods to try. Options: 'boxcox', 'yeo-johnson', 'log'.\n",
    "        plot (bool): If True, plot before/after histograms (for analysis).\n",
    "    Returns:\n",
    "        transformed_series (pd.Series): Transformed feature data (same length as input).\n",
    "        best_method (str): Name of the transformation chosen (or 'none' if no transform improved skew).\n",
    "    \"\"\"\n",
    "    # Calculate original skewness\n",
    "    original_skew = feature_series.skew()\n",
    "    best_method = 'none'\n",
    "    best_skew = abs(original_skew)\n",
    "    best_transformed = feature_series.copy()\n",
    "    \n",
    "    # Try each requested transformation\n",
    "    for method in methods:\n",
    "        transformed = None\n",
    "        try:\n",
    "            if method == 'log':\n",
    "                # Shift data to be positive (if needed) and apply log1p\n",
    "                transformed = np.log1p(feature_series - feature_series.min() + 1)\n",
    "            elif method == 'boxcox':\n",
    "                # Box-Cox requires all positive values\n",
    "                if (feature_series <= 0).any():\n",
    "                    raise ValueError(\"Box-Cox requires positive values\")\n",
    "                transformed, _ = boxcox(feature_series)  # returns transformed data and lambda\n",
    "            elif method == 'yeo-johnson':\n",
    "                pt = PowerTransformer(method='yeo-johnson')\n",
    "                # PowerTransformer returns a 2D array, so reshape is needed\n",
    "                transformed = pt.fit_transform(feature_series.values.reshape(-1, 1)).flatten()\n",
    "            else:\n",
    "                continue  # unknown method\n",
    "        except Exception as e:\n",
    "            # Skip this method if it fails (e.g., boxcox on non-positive data)\n",
    "            continue\n",
    "        \n",
    "        # Compute skew of the transformed data\n",
    "        new_skew = pd.Series(transformed).skew()\n",
    "        if abs(new_skew) < best_skew:\n",
    "            best_skew = abs(new_skew)\n",
    "            best_method = method\n",
    "            best_transformed = pd.Series(transformed, index=feature_series.index)\n",
    "    \n",
    "    # (Optional) plot before vs. after if requested\n",
    "    if plot:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "        axes[0].hist(feature_series, bins=30, color='skyblue')\n",
    "        axes[0].set_title(f\"Original Distribution (skew={original_skew:.2f})\")\n",
    "        axes[1].hist(best_transformed, bins=30, color='orange')\n",
    "        axes[1].set_title(f\"Transformed ({best_method}) (skew={best_skew:.2f})\")\n",
    "        plt.show()\n",
    "    \n",
    "    return best_transformed, best_method\n",
    "\n",
    "# Example usage: fix skewness of 'Amount' feature\n",
    "df['Amount_transformed'], method_used = reduce_skewness(df['Amount'], methods=('log','yeo-johnson','boxcox'))\n",
    "print(f\"Applied {method_used} transform to 'Amount'\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "How this works:\n",
    "- We check the featureâ€™s current skew (original_skew). The function then tries each of the specified methods in turn:\n",
    "    **Log transform (log)**: Uses log1p (log(1+x)) after shifting the data to be positive if needed. Great for right-skewed data with large outliers.\n",
    "    **Box-Cox (boxcox)**: A power transform that only works if all values are positive. It finds an optimal exponent Î» to normalize the distribution.\n",
    "    **Yeo-Johnson (yeo-johnson)**: A power transform similar to Box-Cox but can handle zero or negative values as well.\n",
    "- For each method that succeeds, we compute the absolute skew of the transformed data. The function selects the transformation that yields the lowest skew (i.e., most symmetric distribution). If none of the transforms improve the skew (which is rare), it will return the original data (best_method remains 'none').\n",
    "- Flexible Parameters: You can choose which methods to try via the methods tuple. For example, on a dataset that has only positive values, you might include 'boxcox'; for a dataset with zeros/negatives, use 'yeo-johnson' or 'log'. You can also enable plot=True to visualize the effect for any feature.\n",
    "T- his function handles any numeric feature series. In practice, you would apply it to each numeric column in your dataset that exhibits high skewness (for example, if abs(skew) > 1 as a rule of thumb). You might loop over all numeric features of X and replace those that are significantly skewed with their transformed version. For instance:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:22.796924Z",
     "start_time": "2025-03-02T12:53:22.569493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for col in X.select_dtypes(include='number').columns:\n",
    "    skew_val = X[col].skew()\n",
    "    if abs(skew_val) > 1.0:  # if highly skewed\n",
    "        X[col], method = reduce_skewness(X[col], methods=('log','yeo-johnson','boxcox'))\n",
    "        print(f\"Transformed {col} using {method} (new skew={X[col].skew():.2f})\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "By using reduce_skewness on the **Amount** field in the credit card data, the extreme outliers (very large transactions) were tamed, making the model less biased toward those large values. This approach can be applied to any skewed feature in any dataset."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using the Generic Functions on a New Dataset\n",
    "With these generic functions, you can construct a preprocessing pipeline for any new dataset. For example:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:53:23.294591Z",
     "start_time": "2025-03-02T12:53:22.798939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "# ×¤×•× ×§×¦×™×” ×œ×”×“×ž×™×™×ª ×”×ª×¤×œ×’×•×ª ×”×§×˜×’×•×¨×™×•×ª\n",
    "def plot_class_distribution(y, title):\n",
    "    sns.countplot(x=y)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# ×˜×¢×™× ×ª ×”×“××˜×”×¡×˜\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target  # ×©× ×¢×ž×•×“×ª ×”×ž×˜×¨×” ×”× ×›×•×Ÿ\n",
    "\n",
    "# ×”×¦×’×ª ×”×ª×¤×œ×’×•×ª ×”×ž×—×œ×§×•×ª ×œ×¤× ×™ ××™×–×•×Ÿ\n",
    "plot_class_distribution(df['target'], 'Class Distribution Before Balancing')\n",
    "\n",
    "# ×©×™×ž×•×© ×‘×¤×•× ×§×¦×™×•×ª ×”×›×œ×œ×™×•×ª ×¢× ×©× ×”×¢×ž×•×“×” ×”× ×›×•×Ÿ\n",
    "X_new, y_new = split_features_target(df, target_col=\"target\")\n",
    "\n",
    "# ××™×–×•×Ÿ ×ž×—×œ×§×•×ª\n",
    "X_new_bal, y_new_bal = balance_classes(X_new, y_new, oversample_target=1.0, undersample_factor=1.0)\n",
    "\n",
    "# ×ª×™×§×•×Ÿ ×¡×§×™×•×× ×¡ ×œ×ª×›×•× ×•×ª ×ž×¡×¤×¨×™×•×ª\n",
    "for col in X_new_bal.select_dtypes(include='number').columns:\n",
    "    if abs(X_new_bal[col].skew()) > 1.0:\n",
    "        X_new_bal[col], method = reduce_skewness(X_new_bal[col], methods=('log','yeo-johnson','boxcox'))\n",
    "        print(f\"{col} transformed via {method}\")\n",
    "        \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the snippet above, we demonstrate a full pipeline: splitting data, balancing it, transforming skewed features, scaling, and encoding. Each step is **independent and reusable**. You can mix and match these functions or adjust their parameters as needed:\n",
    "- If a dataset is not imbalanced, you can skip balance_classes or call it with parameters that effectively leave data unchanged.\n",
    "- If features arenâ€™t skewed, reduce_skewness will detect that (no transformation applied if it doesnâ€™t improve skew).\n",
    "- If all features are already numeric and scaled, you might not need scale_features or encode_categoricals.\n",
    "\n",
    "- By converting the specific steps from the credit card dataset into these **flexible functions**, we ensure that the data preprocessing pipeline is not tied to any one dataset. You can apply this pipeline (or its parts) to any new dataset with minimal changes â€” just plug in the dataset and specify relevant columns or thresholds. This makes your data preparation process consistent, modular, and easily adaptable to new problems."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#????????????!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "target_column = 'Class'\n",
    "numerical_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "numerical_features.remove(target_column)\n",
    "# Display dataset shape and first few rows\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(\"\\nSample Rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Set the figure size\n",
    "fig, axes = plt.subplots(nrows=len(numerical_features)//3 + 1, ncols=3, figsize=(18, len(numerical_features)*1.5))\n",
    "\n",
    "# Flatten axes array to iterate over\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histograms for all numerical features\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    sns.histplot(df[feature], bins=100, kde=True, ax=axes[i], color='blue', alpha=0.6)\n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel(\"Count\")\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T12:54:08.577187Z",
     "start_time": "2025-03-02T12:53:23.297603Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:54:08.582421Z",
     "start_time": "2025-03-02T12:54:08.579195Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:54:08.587420Z",
     "start_time": "2025-03-02T12:54:08.584429Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:54:08.593608Z",
     "start_time": "2025-03-02T12:54:08.590427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
