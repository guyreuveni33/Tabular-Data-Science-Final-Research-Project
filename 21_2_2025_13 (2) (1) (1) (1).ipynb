{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Automated Transformation and Optimization for Skewed and Imbalanced Datasets\n",
    "\n",
    "## 0. Introduction\n",
    "This project focuses on improving machine learning model performance on **skewed and imbalanced datasets**. Data imbalance is a major challenge in data science, as standard models tend to favor the majority class, often leading to poor performance when predicting minority class instances.\n",
    "\n",
    "The goal of this work is to develop a **flexible pipeline** that integrates **data preprocessing, model optimization, and performance evaluation**. The approach includes various techniques to address data imbalance, such as **SMOTE, ADASYN, and under-sampling** methods, along with **data transformation techniques like Quantile Transformation and Power Transformation** to adjust feature distributions.\n",
    "\n",
    "Additionally, advanced models such as **LightGBM** are utilized, and model performance is thoroughly assessed using evaluation metrics like **Precision, Recall, Confusion Matrix, and Precision-Recall Curves**. These techniques enhance the model's ability to detect rare cases, which is especially crucial for applications such as **fraud detection, medical diagnosis, and text analysis**.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:30:04.098525Z",
     "start_time": "2025-03-09T19:30:01.453482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import display\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Integrating the Previous Project into the Current Research\n",
    "The previous project successfully demonstrated how class balancing and feature transformations impact model performance. These same principles are applied in the current research, but with a broader focus on automating preprocessing techniques for diverse datasets.\n",
    "\n",
    "- **Class Balancing** (SMOTE & Undersampling): In the wine quality dataset, balancing improved model accuracy by ensuring all classes were well represented. Similarly, in the current research, we apply controlled oversampling and undersampling to create a more robust classification pipeline.\n",
    "\n",
    "- **Skewness Correction**: The previous project applied log transformations to normalize distributions, preventing biases. Here, we expand this with Box-Cox and Yeo-Johnson transformations, adapting preprocessing to different data types.\n",
    "\n",
    "- **Model Optimization**: In the last project, hyperparameter tuning boosted performance dramatically. We extend this by optimizing classification thresholds and cost-sensitive learning to handle imbalanced datasets more effectively."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:30:04.107533Z",
     "start_time": "2025-03-09T19:30:04.100532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Load dataset\n",
    "# data = pd.read_csv(\"winequality-white.csv\", sep=';')\n",
    "#\n",
    "# # Visualize initial class distribution\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# sns.countplot(x='quality', data=data)\n",
    "# plt.title('Wine Quality Distribution - Before Balancing', fontsize=14)\n",
    "# plt.xlabel('Quality', fontsize=12)\n",
    "# plt.ylabel('Frequency', fontsize=12)\n",
    "# plt.show()\n",
    "#\n",
    "# # Split features and target\n",
    "# X = data.drop(columns=['quality'])\n",
    "# y = data['quality']\n",
    "#\n",
    "# # Display initial class distribution\n",
    "# print(\"Class Distribution Before Balancing:\", Counter(y))\n",
    "#\n",
    "# # Step 1: Handle Extremely Rare Classes (Manual Duplication)\n",
    "# min_samples = 30  # Minimum samples required for SMOTE to work\n",
    "#\n",
    "# for cls, count in Counter(y).items():\n",
    "#     if count < min_samples:\n",
    "#         extra_samples = data[data['quality'] == cls].sample(min_samples - count, replace=True)\n",
    "#         data = pd.concat([data, extra_samples])\n",
    "#\n",
    "# # Update X and y after rare class handling\n",
    "# X = data.drop(columns=['quality'])\n",
    "# y = data['quality']\n",
    "#\n",
    "# # Step 2: Apply SMOTE to Underrepresented Classes\n",
    "# max_class_count = max(Counter(y).values())\n",
    "# smote_strategy = {cls: max(count, int(0.6 * max_class_count)) for cls, count in Counter(y).items() if count < 0.6 * max_class_count}\n",
    "#\n",
    "# smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "# X_smote, y_smote = smote.fit_resample(X, y)\n",
    "#\n",
    "# # Display new class distribution after SMOTE\n",
    "# print(\"Class Distribution After SMOTE:\", Counter(y_smote))\n",
    "#\n",
    "# # Visualize class distribution after SMOTE\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# sns.countplot(x=y_smote)\n",
    "# plt.title('Wine Quality Distribution - After SMOTE', fontsize=14)\n",
    "# plt.xlabel('Quality', fontsize=12)\n",
    "# plt.ylabel('Frequency', fontsize=12)\n",
    "# plt.show()\n",
    "#\n",
    "# # Step 3: Apply Proper Undersampling for Overrepresented Classes\n",
    "# min_class_count = min(Counter(y_smote).values())  # Smallest class after SMOTE\n",
    "# undersample_strategy = {}\n",
    "#\n",
    "# for cls, count in Counter(y_smote).items():\n",
    "#     if count > min_class_count * 1.5:  # Reduce dominant classes but keep them slightly higher than others\n",
    "#         undersample_strategy[cls] = int(min_class_count * 1.5)  # Keep majority at 1.5x the smallest class\n",
    "#\n",
    "# # Apply undersampling\n",
    "# undersample = RandomUnderSampler(sampling_strategy=undersample_strategy, random_state=42)\n",
    "# X_final, y_final = undersample.fit_resample(X_smote, y_smote)\n",
    "#\n",
    "# # Display final class distribution after proper balancing\n",
    "# print(\"Final Class Distribution After SMOTE & Corrected Undersampling:\", Counter(y_final))\n",
    "#\n",
    "# # Visualizing target distribution after full balancing\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# sns.countplot(x=y_final)\n",
    "# plt.title('Wine Quality Distribution - After SMOTE & Corrected Undersampling', fontsize=14)\n",
    "# plt.xlabel('Quality', fontsize=12)\n",
    "# plt.ylabel('Frequency', fontsize=12)\n",
    "# plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 How This Improves Prediction?"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:30:04.116045Z",
     "start_time": "2025-03-09T19:30:04.109044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Load the baseline model\n",
    "# baseline_model = joblib.load('baseline_model.pkl')\n",
    "#\n",
    "# # Use the final balanced dataset\n",
    "# X = X_final\n",
    "# y = y_final\n",
    "#\n",
    "# # Split dataset into train and test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#\n",
    "# # Predict on the test set using the baseline model\n",
    "# y_pred_baseline = baseline_model.predict(X_test)\n",
    "#\n",
    "# # Calculate baseline performance metrics\n",
    "# r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "# mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
    "# rmse_baseline = np.sqrt(mse_baseline)\n",
    "#\n",
    "# print(f\"Baseline Model Performance:\\nR^2 Score: {r2_baseline:.4f}\\nMSE: {mse_baseline:.4f}\\nRMSE: {rmse_baseline:.4f}\")\n",
    "#\n",
    "# # Hyperparameter tuning with GridSearchCV for XGBoost\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'subsample': [0.8, 1.0]\n",
    "# }\n",
    "#\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=xgb.XGBRegressor(),\n",
    "#     param_grid=param_grid,\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     cv=5,\n",
    "#     verbose=1\n",
    "# )\n",
    "#\n",
    "# # Fit GridSearch on the balanced dataset\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# best_model = grid_search.best_estimator_\n",
    "# print(f'Best Parameters: {grid_search.best_params_}')\n",
    "#\n",
    "# # Evaluate the improved model\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# rmse = np.sqrt(mse)\n",
    "#\n",
    "# print(f'Improved Model Performance:\\nRMSE: {rmse:.4f}, R^2: {r2:.4f}')\n",
    "#\n",
    "# # Create a comparison table\n",
    "# comparison_df = pd.DataFrame({\n",
    "#     'Metric': ['R^2', 'MSE', 'RMSE'],\n",
    "#     'Baseline Model': [r2_baseline, mse_baseline, rmse_baseline],\n",
    "#     'Improved Model': [r2, mse, rmse]\n",
    "# })\n",
    "#\n",
    "# print(\"\\nComparison of Baseline and Improved Models:\")\n",
    "# print(comparison_df)\n",
    "#\n",
    "# # Plot comparison of R^2\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# bar_width = 0.35\n",
    "# indices = np.arange(len(comparison_df))\n",
    "#\n",
    "# plt.bar(indices, comparison_df['Baseline Model'], bar_width, label='Baseline Model', color='blue')\n",
    "# plt.bar(indices + bar_width, comparison_df['Improved Model'], bar_width, label='Improved Model', color='green')\n",
    "#\n",
    "# plt.xlabel('Metrics')\n",
    "# plt.ylabel('Scores')\n",
    "# plt.title('Comparison of Model Performance')\n",
    "# plt.xticks(indices + bar_width / 2, comparison_df['Metric'])\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The improved model shows a significant enhancement in predictive performance compared to the baseline model. The R² score increased from 0.76 to 0.93, indicating that the new model explains much more variance in the data. The MSE dropped from 0.87 to 0.25, demonstrating a major reduction in prediction errors. Additionally, the RMSE decreased from 0.93 to 0.50, confirming that the model's predictions are more precise and stable.\n",
    "\n",
    "##### **These improvements highlight the impact of data balancing and hyperparameter tuning on predictive accuracy. By applying SMOTE & undersampling, the model now performs better across all quality levels, rather than favoring the dominant ones.**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Broader Impact: Scalable Preprocessing for Various Datasets\n",
    "While the previous project focused on a specific dataset (wine quality), the current research aims to generalize preprocessing techniques to work across multiple data types and domains. By automating class balancing and skew correction, we develop a versatile, adaptable pipeline that enhances machine learning models regardless of dataset characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install lightgbm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-09T19:30:06.222732Z",
     "start_time": "2025-03-09T19:30:04.117555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\noidv\\pycharmprojects\\.venv\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\noidv\\pycharmprojects\\.venv\\lib\\site-packages (from lightgbm) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\noidv\\pycharmprojects\\.venv\\lib\\site-packages (from lightgbm) (1.15.2)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1. Preprocessing the Dataset\n",
    "**Loads the dataset from the specified file path**\n",
    "\n",
    "    - Displays dataset structure and basic statistics."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:30:07.847693Z",
     "start_time": "2025-03-09T19:30:06.224740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(filepath,target_column):\n",
    "    \"\"\"\n",
    "    Loads the credit card fraud dataset, extracts numerical features, and returns\n",
    "    the DataFrame along with a list of numerical columns (excluding the target column).\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): Loaded dataset.\n",
    "        numerical_features (list): List of numerical feature column names (excluding the target).\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Extract numerical features excluding the target column\n",
    "    numerical_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    if target_column in numerical_features:\n",
    "        numerical_features.remove(target_column)\n",
    "\n",
    "    # Display dataset shape and first few rows\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    print(\"\\nSample Rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "    return df, numerical_features  # Return DataFrame and numerical feature names\n",
    "\n",
    "target_column = 'Class'\n",
    "df, numerical_features = load_data(\"creditcard.csv\",target_column)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (284807, 31)\n",
      "\n",
      "Sample Rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:30:07.855147Z",
     "start_time": "2025-03-09T19:30:07.849210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_dtypes = pd.DataFrame(df.dtypes).reset_index()\n",
    "df_dtypes.columns = ['Column', 'Data Type']\n",
    "print(df_dtypes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Column Data Type\n",
      "0     Time   float64\n",
      "1       V1   float64\n",
      "2       V2   float64\n",
      "3       V3   float64\n",
      "4       V4   float64\n",
      "5       V5   float64\n",
      "6       V6   float64\n",
      "7       V7   float64\n",
      "8       V8   float64\n",
      "9       V9   float64\n",
      "10     V10   float64\n",
      "11     V11   float64\n",
      "12     V12   float64\n",
      "13     V13   float64\n",
      "14     V14   float64\n",
      "15     V15   float64\n",
      "16     V16   float64\n",
      "17     V17   float64\n",
      "18     V18   float64\n",
      "19     V19   float64\n",
      "20     V20   float64\n",
      "21     V21   float64\n",
      "22     V22   float64\n",
      "23     V23   float64\n",
      "24     V24   float64\n",
      "25     V25   float64\n",
      "26     V26   float64\n",
      "27     V27   float64\n",
      "28     V28   float64\n",
      "29  Amount   float64\n",
      "30   Class     int64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2. Detecting and Correcting Skewed Features"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:30:08.018782Z",
     "start_time": "2025-03-09T19:30:07.856154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_skewed_features(df, numerical_features, skew_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detects numerical features with skewness above a given threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input dataset.\n",
    "        numerical_features (list): List of numerical feature column names.\n",
    "        skew_threshold (float): The threshold above which features are considered skewed.\n",
    "\n",
    "    Returns:\n",
    "        skewed_features (list): List of skewed numerical feature names.\n",
    "        skewness_table (pd.DataFrame): DataFrame with skewness values for all numerical features.\n",
    "    \"\"\"\n",
    "    print(\"Detecting skewed features...\")\n",
    "\n",
    "    # Compute skewness for numerical features\n",
    "    skewness_values = df[numerical_features].skew()\n",
    "\n",
    "    # Select features where absolute skewness is greater than the threshold\n",
    "    skewed_features = skewness_values[abs(skewness_values) > skew_threshold].index.tolist()\n",
    "\n",
    "    # Store skewness values in a DataFrame for reference\n",
    "    skewness_table = pd.DataFrame({\n",
    "        \"Feature\": numerical_features,\n",
    "        \"Skewness\": skewness_values\n",
    "    }).sort_values(by=\"Skewness\", ascending=False)\n",
    "\n",
    "    print(f\"Skewed Features (>|{skew_threshold}|): {skewed_features}\")\n",
    "\n",
    "    return skewed_features, skewness_table  # Return both the feature names and skewness values\n",
    "\n",
    "# Example usage:\n",
    "skewed_features, skewness_table = detect_skewed_features(df, numerical_features)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting skewed features...\n",
      "Skewed Features (>|0.5|): ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V12', 'V14', 'V16', 'V17', 'V20', 'V21', 'V23', 'V24', 'V26', 'V27', 'V28', 'Amount']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "We would like to identifies numerical features in the dataset that have a skewed distribution. Skewness measures how much a feature deviates from a normal (bell-shaped) distribution. We calculate the skewness of each numerical feature and filter out those with an absolute skewness greater than 0.5, which indicates moderate to high skewness. These identified features may require transformation to improve model performance by ensuring a more normal-like distribution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "#### 3.2.1. Addressing the Issue of Worsened Skewness After Transformation\n",
    "\n",
    "In some cases, applying transformations actually increased the skewness rather than reducing it.\n",
    "To fix this, we will implement a smarter transformation selection approach that avoids making skewness worse.\n",
    "\n",
    "Three Improvements to Prevent Skewness Worsening:\n",
    "1. Add a Post-Transformation Check:\n",
    "If the skewness increases instead of decreasing, we will revert to the original data.\n",
    "2. Choose a More Intelligent Transformation:\n",
    "If all values are strictly positive, we apply Box-Cox.\n",
    "If the feature contains negative values, we use Yeo-Johnson instead.\n",
    "We avoid log transformations automatically, as they may worsen skewness in extreme cases.\n",
    "3. Store the Original Skewness and Compare:\n",
    "After applying a transformation, we check if skewness improved.\n",
    "If the transformation made it worse, we undo the change and restore the original values.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def best_transform(feature_series):\n",
    "    \"\"\"\n",
    "    Try several transformations and select the one that yields the lowest absolute skewness.\n",
    "    Returns the transformed series, the name of the chosen method, the original skew, and the new skew.\n",
    "    \"\"\"\n",
    "    original_skew = feature_series.skew()\n",
    "    best_method = 'none'\n",
    "    best_skew = abs(original_skew)\n",
    "    best_transformed = feature_series.copy()\n",
    "\n",
    "    # Dictionary to store transformation results\n",
    "    transforms = {}\n",
    "\n",
    "    # Try Box-Cox (only if all values are positive)\n",
    "    if (feature_series > 0).all():\n",
    "        try:\n",
    "            transformed, _ = boxcox(feature_series + 1)\n",
    "            skew_val = pd.Series(transformed).skew()\n",
    "            transforms['boxcox'] = (transformed, abs(skew_val))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    # Try Log1p transform (with shift)\n",
    "    try:\n",
    "        transformed = np.log1p(feature_series - feature_series.min() + 1)\n",
    "        skew_val = pd.Series(transformed).skew()\n",
    "        transforms['log1p'] = (transformed, abs(skew_val))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # Try Yeo-Johnson\n",
    "    try:\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "        transformed = pt.fit_transform(feature_series.values.reshape(-1, 1)).flatten()\n",
    "        skew_val = pd.Series(transformed).skew()\n",
    "        transforms['yeo-johnson'] = (transformed, abs(skew_val))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # Try QuantileTransformer (maps data to a normal distribution)\n",
    "    try:\n",
    "        qt = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "        transformed = qt.fit_transform(feature_series.values.reshape(-1, 1)).flatten()\n",
    "        skew_val = pd.Series(transformed).skew()\n",
    "        transforms['quantile'] = (transformed, abs(skew_val))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # Compare all transformations and select the best one if it improves skewness\n",
    "    for method, (trans, skew_val) in transforms.items():\n",
    "        if skew_val < best_skew:\n",
    "            best_skew = skew_val\n",
    "            best_method = method\n",
    "            best_transformed = trans\n",
    "\n",
    "    return best_transformed, best_method, original_skew, pd.Series(best_transformed).skew()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def naive_transform(feature_series):\n",
    "    \"\"\"\n",
    "    A simple, naive approach to handling skewness:\n",
    "    - If skewness > 0.5: Apply log1p transformation.\n",
    "    - If skewness < -0.5: Apply square root transformation.\n",
    "    - If feature contains negatives: Apply min-max scaling.\n",
    "    \"\"\"\n",
    "    original_skew = feature_series.skew()\n",
    "\n",
    "    if (feature_series > 0).all():  # Apply log1p for positive skewed values\n",
    "        transformed = np.log1p(feature_series)\n",
    "        method = \"log1p (naive)\"\n",
    "    elif original_skew < -0.5:  # Apply sqrt for negative skewed values\n",
    "        transformed = np.sqrt(feature_series - feature_series.min() + 1)\n",
    "        method = \"sqrt (naive)\"\n",
    "    else:  # Apply min-max scaling if negative values exist\n",
    "        transformed = (feature_series - feature_series.min()) / (feature_series.max() - feature_series.min())\n",
    "        method = \"min-max (naive)\"\n",
    "\n",
    "    new_skew = pd.Series(transformed).skew()\n",
    "    return transformed, method, original_skew, new_skew"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-09T19:30:08.031145Z",
     "start_time": "2025-03-09T19:30:08.022139Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "**The following transformations on the best Transformation Approach are applied based on methodologies discussed in the article \"Skewness Be Gone: Transformative Tricks for Data Scientists\" by Vinod Chugani.**\n",
    "\n",
    "### **Best Transformation Approach**\n",
    "\n",
    "   ##### **Quantile Transformation**\n",
    "   - **How it works:**\n",
    "     - Maps the feature values to a specified distribution, such as normal, based on their quantiles.\n",
    "     - Effectively addresses skewness by distributing data points evenly across the chosen distribution.\n",
    "   - **Best for:** Severe skewness, non-Gaussian distributions, and handling outliers.\n",
    "\n",
    "   ##### **Yeo-Johnson Transformation**\n",
    "   - **How it works:**\n",
    "     - A power transformation adaptable to both positive and non-positive data.\n",
    "     - Applies an optimal transformation parameter to stabilize variance and reduce skewness.\n",
    "   - **Best for:** Moderate skewness, features with zero or negative values, and when data includes negative or zero values.\n",
    "\n",
    "   ##### **Log1p (Natural Log + 1) Transformation**\n",
    "   - **How it works:**\n",
    "     - Uses a logarithmic transformation to compress large values and reduce right-skewness.\n",
    "     - Applies the natural logarithm to each data point after adding one, effectively handling zero and positive values.\n",
    "   - **Best for:** Mild skewness, right-skewed data, and features where small values dominate.\n",
    "\n",
    "### **Naive Transformation Approach**\n",
    "As a baseline, we implement a **simpler, rule-based approach** to handling skewness. This method applies a **fixed transformation** based on predefined thresholds rather than selecting the optimal transformation.\n",
    "\n",
    "- **Log1p Transformation**: Applied when **skewness > 0.5** (right-skewed).\n",
    "- **Square Root Transformation**: Applied when **skewness < -0.5** (left-skewed).\n",
    "- **Min-Max Scaling**: Applied if the feature contains **negative values**.\n",
    "This approach is computationally simpler but may not always produce the most effective reduction in skewness.\n",
    "\n",
    "\n",
    "\n",
    "### **3.2.2. Applying Data Transformations**\n",
    "To address skewness in our data, we applied several transformations based on methodologies discussed in the article [\"Skewness Be Gone: Transformative Tricks for Data Scientists\" by Vinod Chugani](https://machinelearningmastery.com/skewness-be-gone-transformative-tricks-for-data-scientists/). We compare a **data-driven approach (best transformation selection)** with a **naive approach** to demonstrate the effectiveness of different methods."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:30:19.302665Z",
     "start_time": "2025-03-09T19:30:08.033300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def transform_and_store_skewed_features(df, skewed_features):\n",
    "    \"\"\"\n",
    "    Transforms skewed features using both naive and best approaches.\n",
    "    Stores transformation results in a table for comparison.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    transformed_data = []\n",
    "\n",
    "    for feature in skewed_features:\n",
    "        skew_before = df[feature].skew()\n",
    "\n",
    "        # Best transformation\n",
    "        best_transformed, best_method, best_skew_before, best_skew_after = best_transform(df[feature])\n",
    "\n",
    "        # Naive transformation\n",
    "        naive_transformed, naive_method, naive_skew_before, naive_skew_after = naive_transform(df[feature])\n",
    "\n",
    "        # Store transformation details\n",
    "        transformed_data.append({\n",
    "            \"feature\": feature,\n",
    "            \"original\": df[feature].copy(),\n",
    "            \"best_transformed\": best_transformed,\n",
    "            \"naive_transformed\": naive_transformed,\n",
    "            \"skew_before\": skew_before,\n",
    "            \"best_skew_after\": best_skew_after,\n",
    "            \"naive_skew_after\": naive_skew_after,\n",
    "            \"best_method\": best_method,\n",
    "            \"naive_method\": naive_method\n",
    "        })\n",
    "\n",
    "        results.append([\n",
    "            feature, skew_before, best_skew_after, naive_skew_after, best_method, naive_method\n",
    "        ])\n",
    "\n",
    "        # Apply transformations to df\n",
    "        df[feature] = best_transformed  # Keeping the best transformation\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results, columns=[\n",
    "        \"Feature\", \"Skewness Before\", \"Best Skewness After\", \"Naive Skewness After\",\n",
    "        \"Best Method\", \"Naive Method\"\n",
    "    ])\n",
    "\n",
    "    return transformed_data, results_df\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Detect skewed features\n",
    "# ---------------------------\n",
    "skew_threshold = 0.5\n",
    "numerical_skew = df[numerical_features].skew()\n",
    "skewed_features = numerical_skew[abs(numerical_skew) > skew_threshold].index.tolist()\n",
    "print(\"Skewed Features:\", skewed_features)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Transform Data and Get Results\n",
    "# ---------------------------\n",
    "transformed_data, results_df = transform_and_store_skewed_features(df, skewed_features)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Display the Table FIRST\n",
    "# ---------------------------\n",
    "display(results_df)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewed Features: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V12', 'V14', 'V16', 'V17', 'V20', 'V21', 'V23', 'V24', 'V26', 'V27', 'V28', 'Amount']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Feature  Skewness Before  Best Skewness After  Naive Skewness After  \\\n",
       "0       V1        -3.280667             0.014658             -4.443224   \n",
       "1       V2        -4.624866             0.062620             -7.560717   \n",
       "2       V3        -2.240155             0.057024             -3.390050   \n",
       "3       V4         0.676292             0.018930              0.676292   \n",
       "4       V5        -2.425901             0.016408            -11.843215   \n",
       "5       V6         1.826581            -0.014489              1.826581   \n",
       "6       V7         2.553907             0.017067              2.553907   \n",
       "7       V8        -8.521944            -0.016248            -13.283275   \n",
       "8       V9         0.554680             0.002901              0.554680   \n",
       "9      V10         1.187141             0.000948              1.187141   \n",
       "10     V12        -2.278401             0.006644             -3.738403   \n",
       "11     V14        -1.995176             0.010733             -3.462972   \n",
       "12     V16        -1.100966             0.003737             -2.022841   \n",
       "13     V17        -3.844914             0.012722             -7.890754   \n",
       "14     V20        -2.037155             0.037233             -8.889311   \n",
       "15     V21         3.592991            -0.003685              3.592991   \n",
       "16     V23        -5.875140             0.014924            -14.870339   \n",
       "17     V24        -0.552499            -0.011967             -0.844657   \n",
       "18     V26         0.576693            -0.013798              0.576693   \n",
       "19     V27        -1.170209            -0.070072             -5.061822   \n",
       "20     V28        11.192091            -0.002629             11.192091   \n",
       "21  Amount        16.977724             0.018354             16.977724   \n",
       "\n",
       "    Best Method     Naive Method  \n",
       "0      quantile     sqrt (naive)  \n",
       "1      quantile     sqrt (naive)  \n",
       "2      quantile     sqrt (naive)  \n",
       "3   yeo-johnson  min-max (naive)  \n",
       "4      quantile     sqrt (naive)  \n",
       "5      quantile  min-max (naive)  \n",
       "6      quantile  min-max (naive)  \n",
       "7      quantile     sqrt (naive)  \n",
       "8      quantile  min-max (naive)  \n",
       "9      quantile  min-max (naive)  \n",
       "10     quantile     sqrt (naive)  \n",
       "11     quantile     sqrt (naive)  \n",
       "12     quantile     sqrt (naive)  \n",
       "13     quantile     sqrt (naive)  \n",
       "14     quantile     sqrt (naive)  \n",
       "15     quantile  min-max (naive)  \n",
       "16     quantile     sqrt (naive)  \n",
       "17     quantile     sqrt (naive)  \n",
       "18     quantile  min-max (naive)  \n",
       "19     quantile     sqrt (naive)  \n",
       "20     quantile  min-max (naive)  \n",
       "21  yeo-johnson  min-max (naive)  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Skewness Before</th>\n",
       "      <th>Best Skewness After</th>\n",
       "      <th>Naive Skewness After</th>\n",
       "      <th>Best Method</th>\n",
       "      <th>Naive Method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V1</td>\n",
       "      <td>-3.280667</td>\n",
       "      <td>0.014658</td>\n",
       "      <td>-4.443224</td>\n",
       "      <td>quantile</td>\n",
       "      <td>sqrt (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V2</td>\n",
       "      <td>-4.624866</td>\n",
       "      <td>0.062620</td>\n",
       "      <td>-7.560717</td>\n",
       "      <td>quantile</td>\n",
       "      <td>sqrt (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V3</td>\n",
       "      <td>-2.240155</td>\n",
       "      <td>0.057024</td>\n",
       "      <td>-3.390050</td>\n",
       "      <td>quantile</td>\n",
       "      <td>sqrt (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V4</td>\n",
       "      <td>0.676292</td>\n",
       "      <td>0.018930</td>\n",
       "      <td>0.676292</td>\n",
       "      <td>yeo-johnson</td>\n",
       "      <td>min-max (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V5</td>\n",
       "      <td>-2.425901</td>\n",
       "      <td>0.016408</td>\n",
       "      <td>-11.843215</td>\n",
       "      <td>quantile</td>\n",
       "      <td>sqrt (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>V6</td>\n",
       "      <td>1.826581</td>\n",
       "      <td>-0.014489</td>\n",
       "      <td>1.826581</td>\n",
       "      <td>quantile</td>\n",
       "      <td>min-max (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>V7</td>\n",
       "      <td>2.553907</td>\n",
       "      <td>0.017067</td>\n",
       "      <td>2.553907</td>\n",
       "      <td>quantile</td>\n",
       "      <td>min-max (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>V8</td>\n",
       "      <td>-8.521944</td>\n",
       "      <td>-0.016248</td>\n",
       "      <td>-13.283275</td>\n",
       "      <td>quantile</td>\n",
       "      <td>sqrt (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>V9</td>\n",
       "      <td>0.554680</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.554680</td>\n",
       "      <td>quantile</td>\n",
       "      <td>min-max (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>V10</td>\n",
       "      <td>1.187141</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>1.187141</td>\n",
       "      <td>quantile</td>\n",
       "      <td>min-max (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>V12</td>\n",
       "      <td>-2.278401</td>\n",
       "      <td>0.006644</td>\n",
       "      <td>-3.738403</td>\n",
       "      <td>quantile</td>\n",
       "      <td>sqrt (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>V14</td>\n",
       "      <td>-1.995176</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>-3.462972</td>\n",
       "      <td>quantile</td>\n",
       "      <td>sqrt (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>V16</td>\n",
       "      <td>-1.100966</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>-2.022841</td>\n",
       "      <td>quantile</td>\n",
       "      <td>sqrt (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>V17</td>\n",
       "      <td>-3.844914</td>\n",
       "      <td>0.012722</td>\n",
       "      <td>-7.890754</td>\n",
       "      <td>quantile</td>\n",
       "      <td>sqrt (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>V20</td>\n",
       "      <td>-2.037155</td>\n",
       "      <td>0.037233</td>\n",
       "      <td>-8.889311</td>\n",
       "      <td>quantile</td>\n",
       "      <td>sqrt (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>V21</td>\n",
       "      <td>3.592991</td>\n",
       "      <td>-0.003685</td>\n",
       "      <td>3.592991</td>\n",
       "      <td>quantile</td>\n",
       "      <td>min-max (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>V23</td>\n",
       "      <td>-5.875140</td>\n",
       "      <td>0.014924</td>\n",
       "      <td>-14.870339</td>\n",
       "      <td>quantile</td>\n",
       "      <td>sqrt (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>V24</td>\n",
       "      <td>-0.552499</td>\n",
       "      <td>-0.011967</td>\n",
       "      <td>-0.844657</td>\n",
       "      <td>quantile</td>\n",
       "      <td>sqrt (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>V26</td>\n",
       "      <td>0.576693</td>\n",
       "      <td>-0.013798</td>\n",
       "      <td>0.576693</td>\n",
       "      <td>quantile</td>\n",
       "      <td>min-max (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>V27</td>\n",
       "      <td>-1.170209</td>\n",
       "      <td>-0.070072</td>\n",
       "      <td>-5.061822</td>\n",
       "      <td>quantile</td>\n",
       "      <td>sqrt (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>V28</td>\n",
       "      <td>11.192091</td>\n",
       "      <td>-0.002629</td>\n",
       "      <td>11.192091</td>\n",
       "      <td>quantile</td>\n",
       "      <td>min-max (naive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Amount</td>\n",
       "      <td>16.977724</td>\n",
       "      <td>0.018354</td>\n",
       "      <td>16.977724</td>\n",
       "      <td>yeo-johnson</td>\n",
       "      <td>min-max (naive)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see from the comparison, the **best transformation approach** significantly reduces skewness more effectively than the **naive method**. While the naive approach applies **fixed rules**, the best method selects the **optimal transformation** for each feature, resulting in **lower skewness values** across most cases. This highlights the importance of **choosing the right transformation method** rather than relying on a one-size-fits-all approach."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# # ---------------------------\n",
    "# # Step 3: Plot Histograms for Each Transformed Feature\n",
    "# # ---------------------------\n",
    "# for data in transformed_data:\n",
    "#     feature = data[\"feature\"]\n",
    "#     skew_before = data[\"skew_before\"]\n",
    "#     best_skew_after = data[\"best_skew_after\"]\n",
    "#     naive_skew_after = data[\"naive_skew_after\"]\n",
    "#\n",
    "#     fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # 1 Row, 3 Columns\n",
    "#\n",
    "#     # Histogram Before Transformation\n",
    "#     sns.histplot(data[\"original\"], bins=100, kde=True, ax=axes[0], color='blue', alpha=0.6)\n",
    "#     axes[0].set_title(f'Before Transformation: {feature} (Skew: {skew_before:.2f})')\n",
    "#\n",
    "#     # Histogram After Best Transformation\n",
    "#     sns.histplot(data[\"best_transformed\"], bins=100, kde=True, ax=axes[1], color='green', alpha=0.6)\n",
    "#     axes[1].set_title(f'Best Transformation: {feature} (Skew: {best_skew_after:.2f})')\n",
    "#\n",
    "#     # Histogram After Naive Transformation\n",
    "#     sns.histplot(data[\"naive_transformed\"], bins=100, kde=True, ax=axes[2], color='orange', alpha=0.6)\n",
    "#     axes[2].set_title(f'Naive Transformation: {feature} (Skew: {naive_skew_after:.2f})')\n",
    "#\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-09T19:30:19.307903Z",
     "start_time": "2025-03-09T19:30:19.303671Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this table, we see the impact of our **automated transformation process** on the **skewness of numerical features**. The goal of these transformations is to **reduce skewness** and make the feature distributions more **normal-like**, which improves the performance of machine learning models.\n",
    "\n",
    "#### 3.2.3. Observations & Improvements\n",
    "- **Most features had a high initial skewness**, either negative or positive.\n",
    "- **The process successfully reduced skewness for all features**, making them more normally distributed.\n",
    "- **Different transformation methods were applied**:\n",
    "  - **Quantile Transformation** was the most commonly chosen method, as it is effective for severe skewness.\n",
    "  - **Yeo-Johnson Transformation** was applied for `Amount` and `V4`, as it worked better in reducing skewness for these features.\n",
    "\n",
    "#### 3.2.4. Why Does This Matter?\n",
    "- **Reducing skewness** helps **stabilize variance** and improves the **effectiveness of machine learning models**.\n",
    "- **Highly skewed features** can lead to models being **biased** toward extreme values.\n",
    "- By using **appropriate transformations**, we ensure that features contribute **meaningfully** to model predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:30:19.317908Z",
     "start_time": "2025-03-09T19:30:19.308910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # ---------------------------\n",
    "# # Step 3: Plot Only the Last Transformed Feature with Boxplots\n",
    "# # ---------------------------\n",
    "# if transformed_data:  # Ensure there's at least one feature\n",
    "#     last_feature_data = transformed_data[-1]  # Get last transformed feature\n",
    "#\n",
    "#     feature = last_feature_data[\"feature\"]\n",
    "#     skew_before = last_feature_data[\"skew_before\"]\n",
    "#     best_skew_after = last_feature_data[\"best_skew_after\"]\n",
    "#     naive_skew_after = last_feature_data[\"naive_skew_after\"]\n",
    "#\n",
    "#     fig, axes = plt.subplots(3, 2, figsize=(20, 16))  # 3 Rows, 2 Columns\n",
    "#\n",
    "#     # Histogram Before Transformation\n",
    "#     sns.histplot(last_feature_data[\"original\"], bins=100, kde=True, ax=axes[0, 0], color='blue', alpha=0.6)\n",
    "#     axes[0, 0].set_title(f'Before Transformation: {feature} (Skew: {skew_before:.2f})')\n",
    "#\n",
    "#     # Boxplot Before Transformation\n",
    "#     sns.boxplot(x=last_feature_data[\"original\"], ax=axes[0, 1], color='cyan')\n",
    "#     axes[0, 1].set_title(f'Boxplot Before: {feature}')\n",
    "#\n",
    "#     # Histogram After Best Transformation\n",
    "#     sns.histplot(last_feature_data[\"best_transformed\"], bins=100, kde=True, ax=axes[1, 0], color='green', alpha=0.6)\n",
    "#     axes[1, 0].set_title(f'Best Transformation: {feature} (Skew: {best_skew_after:.2f})')\n",
    "#\n",
    "#     # Boxplot After Best Transformation\n",
    "#     sns.boxplot(x=last_feature_data[\"best_transformed\"], ax=axes[1, 1], color='lime')\n",
    "#     axes[1, 1].set_title(f'Boxplot After Best Transformation: {feature}')\n",
    "#\n",
    "#     # Histogram After Naive Transformation\n",
    "#     sns.histplot(last_feature_data[\"naive_transformed\"], bins=100, kde=True, ax=axes[2, 0], color='orange', alpha=0.6)\n",
    "#     axes[2, 0].set_title(f'Naive Transformation: {feature} (Skew: {naive_skew_after:.2f})')\n",
    "#\n",
    "#     # Boxplot After Naive Transformation\n",
    "#     sns.boxplot(x=last_feature_data[\"naive_transformed\"], ax=axes[2, 1], color='red')\n",
    "#     axes[2, 1].set_title(f'Boxplot After Naive Transformation: {feature}')\n",
    "#\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3. Conclusions from the Transformation Analysis\n",
    "**Before Transformation (Left Graph):**\n",
    "\n",
    "The feature \"Amount\" had a high skewness (16.98), meaning the distribution was heavily skewed to the right.\n",
    "A large concentration of values on the left side, with extreme outliers on the right.\n",
    "\n",
    "**Boxplot (Middle Graph):**\n",
    "\n",
    "Most values are clustered near the lower end of the range.\n",
    "A significant number of outliers are visible, indicating extreme variations in the data.\n",
    "\n",
    "**After Transformation (Right Graph):**\n",
    "\n",
    "Skewness significantly reduced (0.32), bringing the distribution closer to normal.\n",
    "The data is now more evenly spread, with reduced impact from extreme values.\n",
    "\n",
    "### 3.4 Key Takeaways:\n",
    "\n",
    "   - The transformation successfully reduced skewness, making the data more suitable for machine learning models.\n",
    "   - The feature now has a smoother distribution, which can improve model performance.\n",
    "   - Although the transformation did not fully normalize the data, it represents a significant improvement over the original distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The process of detecting and transforming skewed features aligns with the principles of **moment skewness** and **standardized moments** discussed in **Lecture 4 on our course**. The lecture explained how **different types of skewness measures exist**, including **absolute skewness, non-parametric skewness, and moment-based skewness**. It highlights that skewed data distributions lead to **biased statistical inferences**, which can negatively impact predictive models. Our approach addresses this by first detecting skewed features using a **numerical skewness threshold**, then applying **optimal transformations** to bring feature distributions closer to normality, reducing bias and improving model performance.\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Handling Imbalanced Data in Machine Learning\n",
    "\n",
    "In many real-world datasets, class distributions are often **highly imbalanced**, meaning that one class (the majority) significantly outnumbers another (the minority). This imbalance can lead to **biased machine learning models**, where classifiers tend to favor the majority class and overlook the minority instances. This problem is particularly critical in applications such as fraud detection, rare disease diagnosis, and customer churn prediction, where the minority class holds the most valuable insights.\n",
    "\n",
    "To solve this, we combine **TF-IDF vectorization with SMOTE and undersampling**, ensuring that:\n",
    "\n",
    "    - Text features are first extracted using TF-IDF to transform words into numerical representations.\n",
    "    - Class imbalance is handled using SMOTE and undersampling to ensure fair learning.\n",
    "\n",
    "\n",
    "A combination of both techniques is often the most effective strategy **(Batista et al., 2004)**. Below, we introduce SMOTE (Synthetic Minority Over-sampling Technique) and Undersampling, discuss their variations, and provide an optimal approach for their combined usage.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.1 Apply TF-IDF for Text Processing\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a numerical statistic used to evaluate the importance of a word in a document relative to a collection of documents **(Salton & Buckley, 1988)**. Instead of assigning equal weight to every word, **TF-IDF prioritizes words that are more distinctive within a document but not overly common across all documents**.\n",
    "\n",
    "**Why Use TF-IDF?**\n",
    "\n",
    "    - Removes common stopwords that appear frequently but contribute little meaning.\n",
    "    - Emphasizes unique words that help distinguish between different documents.\n",
    "    - Outperforms simple word frequency (Bag-of-Words) in many text classification tasks.\n",
    "    \n",
    "**Research on TF-IDF in Imbalanced Datasets**\n",
    "- **Joachims (1998)** found that TF-IDF significantly improved text classification accuracy in Support Vector Machines (SVMs).\n",
    "- **Zhang et al. (2020)** showed that combining TF-IDF with SMOTE enhanced text classification performance in low-resource NLP tasks.\n",
    "- **Huang et al. (2019)** demonstrated that undersampling majority texts after TF-IDF transformation helped balance class distribution.\n",
    "\n",
    "**TF-IDF Implementation**\n",
    "\n",
    "Here’s an improved TF-IDF transformation function that ensures compatibility with resampling techniques."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:30:19.332245Z",
     "start_time": "2025-03-09T19:30:19.319424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_tfidf(data, text_column, max_features=5000, ngram_range=(1,2), stop_words='english'):\n",
    "    \"\"\"\n",
    "    Applies TF-IDF transformation to a text column in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame containing the dataset\n",
    "    - text_column: Name of the column containing text data\n",
    "    - max_features: Maximum number of TF-IDF features to extract (default: 5000)\n",
    "    - ngram_range: Tuple defining n-gram range (default: (1,2) for unigrams and bigrams)\n",
    "    - stop_words: Stopword removal ('english' removes standard stopwords)\n",
    "\n",
    "    Returns:\n",
    "    - Transformed DataFrame with TF-IDF features replacing the original text column.\n",
    "    \"\"\"\n",
    "    print(f\"\\n Applying TF-IDF on column: {text_column}\")\n",
    "\n",
    "    # Initialize TF-IDF vectorizer with optimized parameters\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range, stop_words=stop_words)\n",
    "    tfidf_matrix = vectorizer.fit_transform(data[text_column])\n",
    "\n",
    "    # Convert to DataFrame with feature names\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    print(f\"Extracted {len(tfidf_df.columns)} TF-IDF features for column: {text_column}\")\n",
    "\n",
    "    # Drop original text column and merge with transformed features\n",
    "    data = data.drop(columns=[text_column]).reset_index(drop=True)\n",
    "    data = pd.concat([data, tfidf_df], axis=1)\n",
    "\n",
    "    print(f\"New dataset shape after TF-IDF transformation: {data.shape}\")\n",
    "\n",
    "    return data\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:30:19.341257Z",
     "start_time": "2025-03-09T19:30:19.334254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Detect all text columns dynamically\n",
    "text_columns = df.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "\n",
    "if text_columns:\n",
    "    print(f\"\\nDetected text columns for TF-IDF transformation: {text_columns}\")\n",
    "else:\n",
    "    print(\"\\nNo text columns detected. Skipping TF-IDF transformation.\")\n",
    "    \n",
    "\n",
    "# Apply TF-IDF transformation if text columns exist\n",
    "for col in text_columns:\n",
    "    df = apply_tfidf(df, col)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No text columns detected. Skipping TF-IDF transformation.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**What Happens Here?**\n",
    "\n",
    "Automatically finds text columns → No need to manually specify them.\n",
    "\n",
    "Applies TF-IDF transformation → Converts words into numerical values.\n",
    "\n",
    "Ensures compatibility with SMOTE → Prevents errors when handling imbalanced data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2 Handling Imbalanced Datasets with SMOTE and Undersampling\n",
    "initially had a highly imbalanced class distribution:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:30:19.391344Z",
     "start_time": "2025-03-09T19:30:19.342379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature and target split\n",
    "X = df.drop(columns=[target_column])  # Features\n",
    "y = df[target_column]  # Target variable\n",
    "\n",
    "# Display class distribution before balancing\n",
    "print(\"Class Distribution Before Balancing:\", Counter(y))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Before Balancing: Counter({0: 284315, 1: 492})\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Class Distribution Before Balancing:\n",
    "\n",
    "    - Class 0 (No Fraud): 284,315 samples\n",
    "    - Class 1 (Fraud): Only 492 samples!\n",
    "This is extremely imbalanced!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.2. Handling Imbalanced Datasets with Smart Resampling\n",
    "\n",
    "This section explores the use of resampling techniques, particularly the combination of **SMOTE (Synthetic Minority Over-sampling Technique)** with undersampling methods such as **Tomek Links and Edited Nearest Neighbors (ENN)**, to handle imbalanced datasets. The discussion is supported by findings from recent academic research.\n",
    "\n",
    "#### 4.2.1. Background on Data Imbalance\n",
    "Data imbalance occurs when one class significantly outnumbers another, leading machine learning models to develop a bias toward the dominant class. This issue is prevalent in domains such as fraud detection, medical diagnosis, and rare event prediction.\n",
    "\n",
    "#### 4.2.2. Oversampling Techniques: SMOTE and ADASYN\n",
    "- **SMOTE (Synthetic Minority Over-sampling Technique):**  \n",
    "  Generates synthetic samples by interpolating between existing minority class instances, increasing the representation of the minority class while maintaining natural data distribution.\n",
    "\n",
    "- **ADASYN (Adaptive Synthetic Sampling Approach for Imbalanced Learning):**  \n",
    "  A more advanced technique that **focuses on difficult-to-learn** minority instances, generating synthetic data in a **weighted manner** to enhance classification performance.\n",
    "\n",
    "#### 4.2.3. Undersampling Techniques: Tomek Links and ENN\n",
    "- **Tomek Links:**  \n",
    "  Identifies and removes **pairs of nearest samples** from different classes that are too close to each other, thereby **enhancing class separability** and reducing noise.\n",
    "\n",
    "- **Edited Nearest Neighbors (ENN):**  \n",
    "  Removes instances that are **misclassified by their nearest neighbors**, leading to a **cleaner dataset with more reliable decision boundaries**.\n",
    "\n",
    "#### 4.2.4. Combining SMOTE with Tomek Links or ENN**\n",
    "Research indicates that **combining SMOTE with undersampling techniques** provides **optimal balance** between increasing minority class representation and reducing noise in the dataset. Studies have demonstrated that this hybrid approach improves model performance and **reduces classification errors** ((https://www.researchgate.net/publication/360157856_Tomek_Link_and_SMOTE_Approaches_for_Machine_Fault_Classification_with_an_Imbalanced_Dataset?utm_source=chatgpt.com)).\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2.5. Implementation of naive functions for evaluation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:41:22.164981Z",
     "start_time": "2025-03-09T19:30:19.392351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "\n",
    "# Naive SMOTE function\n",
    "def naive_smote(X, y, random_state=42):\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_res, y_res = smote.fit_resample(X, y)\n",
    "    print(\"Naive SMOTE Class Distribution:\", Counter(y_res))\n",
    "    return X_res, y_res\n",
    "\n",
    "# Naive Undersampling function\n",
    "def naive_undersampling(X, y):\n",
    "    undersampler = TomekLinks()\n",
    "    X_res, y_res = undersampler.fit_resample(X, y)\n",
    "    print(\"Naive Undersampling Class Distribution:\", Counter(y_res))\n",
    "    return X_res, y_res\n",
    "\n",
    "\n",
    "# Apply naive methods\n",
    "X_smote, y_smote = naive_smote(X, y)\n",
    "X_under, y_under = naive_undersampling(X_smote, y_smote)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive SMOTE Class Distribution: Counter({0: 284315, 1: 284315})\n",
      "Naive Undersampling Class Distribution: Counter({0: 284315, 1: 284113})\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2.6. Implementation in `auto_balance_pipeline`**\n",
    "The `auto_balance_pipeline` function dynamically selects appropriate resampling techniques based on the severity of class imbalance:\n",
    "\n",
    "- **For highly imbalanced datasets**, **ADASYN** is preferred over SMOTE to **generate more realistic synthetic samples**.  \n",
    "- **After oversampling**, **Tomek Links or ENN** are applied to remove **noisy majority class samples**, refining decision boundaries.  \n",
    "\n",
    "This process ensures that the dataset remains **balanced without unnecessary over-sampling** while also improving model generalization."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:49:49.905963Z",
     "start_time": "2025-03-09T19:41:22.166223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "# Function to visualize class distribution\n",
    "def plot_class_distribution(y_before, y_after):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.countplot(x=y_before, ax=axes[0], palette=\"coolwarm\")\n",
    "    axes[0].set_title(\"Original Class Distribution\")\n",
    "    sns.countplot(x=y_after, ax=axes[1], palette=\"coolwarm\")\n",
    "    axes[1].set_title(\"Resampled Class Distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Smart balancing pipeline\n",
    "def auto_balance_pipeline(X, y, model=None, sampling_ratio=0.25, random_state=42):\n",
    "    class_counts = Counter(y)\n",
    "    majority_class = max(class_counts, key=class_counts.get)\n",
    "    minority_class = min(class_counts, key=class_counts.get)\n",
    "    imbalance_ratio = class_counts[majority_class] / class_counts[minority_class]\n",
    "\n",
    "    print(f\"\\nOriginal Class Distribution: {class_counts}\")\n",
    "    print(f\"Imbalance Ratio: {imbalance_ratio:.2f}\")\n",
    "\n",
    "    # Choose SMOTE variant\n",
    "    smote = ADASYN(sampling_strategy=sampling_ratio, random_state=random_state) if imbalance_ratio > 10 else SMOTE(sampling_strategy=sampling_ratio, random_state=random_state)\n",
    "    undersampler = TomekLinks() if imbalance_ratio > 20 else EditedNearestNeighbours(n_neighbors=3)\n",
    "\n",
    "    print(\"Applying Oversampling & Undersampling...\")\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    X_resampled, y_resampled = undersampler.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "    print(f\"Resampled Class Distribution: {Counter(y_resampled)}\")\n",
    "    plot_class_distribution(y, y_resampled)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "X_smart, y_smart = auto_balance_pipeline(X, y)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Class Distribution: Counter({0: 284315, 1: 492})\n",
      "Target Sample Counts: {0: 284315, 1: 213236}\n",
      "\n",
      "Applying Oversampling...\n",
      "After SMOTE: Counter({0: 284315, 1: 213019})\n",
      "\n",
      "Applying Undersampling...\n",
      "After Undersampling: Counter({0: 284092, 1: 213019})\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2.7. Compare distributions"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:49:53.301824Z",
     "start_time": "2025-03-09T19:49:49.908975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def compare_distributions(y_original, y_smote, y_under, y_smart):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    sns.countplot(x=y_original, ax=axes[0], palette=\"coolwarm\")\n",
    "    axes[0].set_title(\"Original Data\")\n",
    "    sns.countplot(x=y_under, ax=axes[2], palette=\"coolwarm\")\n",
    "    axes[1].set_title(\"Naive SMOTE & Undersampling\")\n",
    "    sns.countplot(x=y_smart, ax=axes[3], palette=\"coolwarm\")\n",
    "    axes[2].set_title(\"Smart Resampling (SMOTE + Under)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_distributions(y, y_smote, y_under, y_smart)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noidv\\AppData\\Local\\Temp\\ipykernel_27964\\1307472160.py:3: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x=y_original, ax=axes[0], palette=\"coolwarm\")\n",
      "C:\\Users\\noidv\\AppData\\Local\\Temp\\ipykernel_27964\\1307472160.py:5: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x=y_under, ax=axes[2], palette=\"coolwarm\")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m     plt\u001B[38;5;241m.\u001B[39mtight_layout()\n\u001B[0;32m     10\u001B[0m     plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m---> 12\u001B[0m \u001B[43mcompare_distributions\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_smote\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_under\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_smart\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[17], line 7\u001B[0m, in \u001B[0;36mcompare_distributions\u001B[1;34m(y_original, y_smote, y_under, y_smart)\u001B[0m\n\u001B[0;32m      5\u001B[0m sns\u001B[38;5;241m.\u001B[39mcountplot(x\u001B[38;5;241m=\u001B[39my_under, ax\u001B[38;5;241m=\u001B[39maxes[\u001B[38;5;241m2\u001B[39m], palette\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoolwarm\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m axes[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mset_title(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNaive SMOTE & Undersampling\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m sns\u001B[38;5;241m.\u001B[39mcountplot(x\u001B[38;5;241m=\u001B[39my_smart, ax\u001B[38;5;241m=\u001B[39m\u001B[43maxes\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m, palette\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoolwarm\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      8\u001B[0m axes[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m.\u001B[39mset_title(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSmart Resampling (SMOTE + Under)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      9\u001B[0m plt\u001B[38;5;241m.\u001B[39mtight_layout()\n",
      "\u001B[1;31mIndexError\u001B[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x500 with 3 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Class Distribution Comparison Across Different Resampling Methods**\n",
    "\n",
    "This figure presents the class distribution before and after applying different resampling techniques:\n",
    "\n",
    "1. **Original Data (Leftmost Plot)**:\n",
    "\n",
    "    - The dataset is **highly imbalanced**, with the majority class (0) significantly outnumbering the minority class (1).\n",
    "    - This imbalance can lead to biased models that favor the majority class.\n",
    "        \n",
    "2. **Naive SMOTE (Second Plot)**:\n",
    "\n",
    "    - SMOTE **generates synthetic samples** for the minority class until both classes are of equal size.\n",
    "    - **Issue**: The dataset is now artificially balanced but may include redundant or unrealistic samples, increasing the risk of overfitting.\n",
    "        \n",
    "3. **Naive Undersampling (Third Plot)**:\n",
    "\n",
    "    - Undersampling **removes samples from the majority** class to balance the dataset.\n",
    "    - **Issue**: The dataset remains imbalanced, and useful majority class data is lost, which can degrade model performance.\n",
    "        \n",
    "4. **Smart Resampling (Rightmost Plot - SMOTE + Undersampling)**:\n",
    "\n",
    "    - A combination of **SMOTE and undersampling** ensures a more natural balance between the two classes.\n",
    "    - Unlike naive SMOTE, it does not over-generate synthetic samples, and unlike naive undersampling, it retains valuable majority class instances.\n",
    "    - This approach **reduces bias while maintaining the integrity of the dataset**, leading to better generalization in classification models."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:49:53.304828Z",
     "start_time": "2025-03-09T19:49:53.304828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(X_orig, y_orig, X_resampled, y_resampled, model):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42)\n",
    "    X_res_train, X_res_test, y_res_train, y_res_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_orig = model.predict(X_test)\n",
    "    print(\"\\nPerformance Before Resampling:\")\n",
    "    print(classification_report(y_test, y_pred_orig))\n",
    "    model.fit(X_res_train, y_res_train)\n",
    "    y_pred_res = model.predict(X_res_test)\n",
    "    print(\"\\nPerformance After Resampling:\")\n",
    "    print(classification_report(y_res_test, y_pred_res))\n",
    "    \n",
    "evaluate_model(X, y, X_smart, y_smart, RandomForestClassifier(random_state=42))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Interpretation of Model Performance Before and After Resampling**\n",
    "\n",
    "The classification report provides insight into how resampling affected model performance. Below is an analysis of the results:\n",
    "\n",
    "- **Before Resampling**: The model achieved **high accuracy (1.00)** but was trained on an **imbalanced dataset** (minority class = 969 samples).\n",
    "- **After Resampling**: Minority class samples increased to **4,692**, improving representation while maintaining **perfect precision and recall**.\n",
    "- **Key Insight**: The model remains highly accurate **without overfitting**, confirming that **smart resampling improves class balance without degrading performance**. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_pca_comparison(X_orig, y_orig, X_naive, y_naive, X_smart, y_smart):\n",
    "    pca = PCA(n_components=2)\n",
    "    \n",
    "    X_pca_orig = pca.fit_transform(X_orig)\n",
    "    X_pca_naive = pca.transform(X_naive)\n",
    "    X_pca_smart = pca.transform(X_smart)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # גרף 1: נתונים מקוריים\n",
    "    axes[0].scatter(X_pca_orig[:, 0], X_pca_orig[:, 1], c=y_orig, cmap=\"coolwarm\", alpha=0.3)\n",
    "    axes[0].set_title(\"Original Data\")\n",
    "\n",
    "    # גרף 2: SMOTE נאיבי\n",
    "    axes[1].scatter(X_pca_naive[:, 0], X_pca_naive[:, 1], c=y_naive, cmap=\"coolwarm\", alpha=0.3)\n",
    "    axes[1].set_title(\"Naive SMOTE (Only Oversampling)\")\n",
    "\n",
    "    # גרף 3: הפייפליין החכם שלך\n",
    "    axes[2].scatter(X_pca_smart[:, 0], X_pca_smart[:, 1], c=y_smart, cmap=\"coolwarm\", alpha=0.3)\n",
    "    axes[2].set_title(\"Smart Resampling (SMOTE + Undersampling)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# הרצת ההשוואה\n",
    "plot_pca_comparison(X, y, X_smote, y_smote, X_smart, y_smart)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**PCA Visualization Comparing Different Resampling Methods**\n",
    "\n",
    "This figure presents a PCA visualization comparing the original dataset, Naive SMOTE (oversampling only), and Smart Resampling (SMOTE + Undersampling).\n",
    "\n",
    "- **Original Data (Left)**: The majority class (blue) is widely distributed, while the minority class (red) is concentrated in a limited area.\n",
    "- **Naive SMOTE (Middle)**: Synthetic samples are generated in a **linear fashion**, significantly expanding the minority class but in an **unnatural and uniform** manner. This may lead to **overfitting***, as the added samples do not realistically represent the data distribution.\n",
    "- **Smart Resampling (Right)**: The combination of **SMOTE and undersampling** ensures that synthetic samples are added **strategically**, preserving the data structure while reducing noise. This approach improves **class separation** and **model generalization** compared to naive oversampling.\n",
    "\n",
    "By integrating both oversampling and undersampling, the Smart Resampling method maintains a **better-balanced dataset** with **fewer redundant or misleading synthetic samples**, leading to improved classifier performance."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 4.2.5. Advantages of This Approach\n",
    "✔ **Improved Model Performance:**  \n",
    "   - Reduces the risk of **overfitting** by preventing excessive synthetic sample generation.  \n",
    "   - Enhances **decision boundaries** by removing misleading majority class instances.  \n",
    "\n",
    "✔ **Preserving Important Information:**  \n",
    "   - Unlike naive SMOTE, **this method retains meaningful patterns** while eliminating **redundant or noisy data points**.  \n",
    "\n",
    "#### 4.2.6 **Conclusion**\n",
    "Combining **SMOTE with undersampling techniques** such as **Tomek Links or ENN** provides an effective solution for handling imbalanced datasets. This approach **improves class representation while maintaining data integrity**, leading to **better classifier performance in real-world applications**."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. LightGBM Results on Imbalanced vs. Balanced Datasets"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:49:53.307829Z",
     "start_time": "2025-03-09T19:49:53.307829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_evaluate_lgbm(X, y, X_balanced, y_balanced, test_size=0.2, train_size=0.3, random_state=42):\n",
    "    # Split the imbalanced dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, train_size=train_size, random_state=random_state, stratify=y)\n",
    "\n",
    "    # Split the balanced dataset\n",
    "    X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(\n",
    "        X_balanced, y_balanced, test_size=test_size, train_size=train_size, random_state=random_state, stratify=y_balanced\n",
    "    )\n",
    "\n",
    "    # Define LightGBM classifier parameters\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'random_state': random_state\n",
    "    }\n",
    "\n",
    "    # Train LightGBM on imbalanced dataset\n",
    "    start_time = time.time()\n",
    "    model_lgb = lgb.LGBMClassifier(**lgb_params)\n",
    "    model_lgb.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Predictions on imbalanced test data\n",
    "    y_pred_lgb = model_lgb.predict(X_test)\n",
    "    y_probs_lgb = model_lgb.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "    # Evaluate performance on imbalanced dataset\n",
    "    print(f\"Performance on Imbalanced Dataset (Training Time: {end_time - start_time:.2f} sec):\")\n",
    "    print(classification_report(y_test, y_pred_lgb))\n",
    "\n",
    "    # Train LightGBM on balanced dataset\n",
    "    start_time = time.time()\n",
    "    model_lgb_bal = lgb.LGBMClassifier(**lgb_params)\n",
    "    model_lgb_bal.fit(X_train_bal, y_train_bal)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Predictions on balanced test data\n",
    "    y_pred_bal_lgb = model_lgb_bal.predict(X_test_bal)\n",
    "    y_probs_bal_lgb = model_lgb_bal.predict_proba(X_test_bal)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "    # Evaluate performance on balanced dataset\n",
    "    print(f\"Performance on Balanced Dataset (Training Time: {end_time - start_time:.2f} sec):\")\n",
    "    print(classification_report(y_test_bal, y_pred_bal_lgb))\n",
    "\n",
    "    return y_test, y_pred_lgb, y_probs_lgb, y_test_bal, y_pred_bal_lgb, y_probs_bal_lgb\n",
    "\n",
    "# Example call to the function\n",
    "y_test, y_pred_lgb, y_probs_lgb, y_test_bal, y_pred_bal_lgb, y_probs_bal_lgb = train_evaluate_lgbm(X, y, X_smart, y_smart)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1 Understanding the Context\n",
    "The output shows the results of **LightGBM (a gradient boosting model)** trained on both an **imbalanced dataset** and a **balanced dataset**. The dataset appears to be a **binary classification task** (fraud detection or another imbalanced scenario) with labels:\n",
    "- **Class 0** (Negative class, majority)\n",
    "- **Class 1** (Positive class, minority)\n",
    "\n",
    "The model's performance is measured using **precision, recall, f1-score, and accuracy** on each dataset.\n",
    "\n",
    "\n",
    "### 5.2. Imbalanced Dataset Training Results\n",
    "```\n",
    "[LightGBM] [Info] Number of positive: 148, number of negative: 85294\n",
    "```\n",
    "- **148 fraud cases (positive class)**\n",
    "- **85,294 non-fraud cases (negative class)**\n",
    "- **Highly imbalanced (~0.17% fraud cases)**\n",
    "\n",
    "#### Training Info\n",
    "```\n",
    "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007051 seconds.\n",
    "[LightGBM] [Info] Total Bins 7650\n",
    "[LightGBM] [Info] Number of data points in the train set: 85442, number of used features: 30\n",
    "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001732 -> initscore=-6.356647\n",
    "[LightGBM] [Info] Start training from score -6.356647\n",
    "```\n",
    "- **Training on 85,442 samples** with **30 features**\n",
    "- The prior probability of fraud cases (`pavg=0.001732`) is extremely low.\n",
    "- The **initial model score** for LightGBM is **-6.356647**, meaning it's starting heavily biased toward the majority class.\n",
    "\n",
    "#### Performance on Imbalanced Dataset\n",
    "```\n",
    "Performance on Imbalanced Dataset (Training Time: 0.49 sec):\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00     56864\n",
    "           1       0.50      0.65      0.57        98\n",
    "\n",
    "    accuracy                           1.00     56962\n",
    "   macro avg       0.75      0.83      0.78     56962\n",
    "weighted avg       1.00      1.00      1.00     56962\n",
    "```\n",
    "\n",
    "#### **Key Observations\n",
    "- **Accuracy = 1.00 (~100%)** → **Misleading due to class imbalance**\n",
    "- **Class 0 (Majority, non-fraud)**: Precision, recall, and F1-score are all **1.00** (perfect) because almost all predictions are for Class 0.\n",
    "- **Class 1 (Minority, fraud cases)**:\n",
    "  - **Precision = 0.50** → Only 50% of predicted fraud cases were actually fraud.\n",
    "  - **Recall = 0.65** → The model correctly detected 65% of actual fraud cases but still missed 35%.\n",
    "  - **F1-score = 0.57** → Imbalanced performance; the model struggles with fraud detection.\n",
    "\n",
    "#### Problem with Imbalanced Data\n",
    "- The model is highly **biased toward the majority class**.\n",
    "- Fraud cases are often **undetected or misclassified**.\n",
    "- **Precision (0.50) is low**, meaning there are a lot of false positives.\n",
    "- **Recall (0.65) is also not great**, meaning some fraud cases are missed.\n",
    "\n",
    "\n",
    "\n",
    "### 5.3. Balanced Dataset Training Results\n",
    "```\n",
    "[LightGBM] [Info] Number of positive: 8558, number of negative: 17115\n",
    "```\n",
    "- **8,558 fraud cases**\n",
    "- **17,115 non-fraud cases**\n",
    "- **Balanced dataset (1:2 ratio of fraud to non-fraud cases)**\n",
    "\n",
    "#### Training Info\n",
    "```\n",
    "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002783 seconds.\n",
    "[LightGBM] [Info] Total Bins 7650\n",
    "[LightGBM] [Info] Number of data points in the train set: 25673, number of used features: 30\n",
    "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.333346 -> initscore=-0.693089\n",
    "[LightGBM] [Info] Start training from score -0.693089\n",
    "```\n",
    "- **Training on 25,673 samples** (less than the imbalanced dataset, due to balancing techniques).\n",
    "- The prior probability of fraud cases (`pavg=0.333346`) is **significantly higher** than in the imbalanced dataset.\n",
    "- **Initial model score (-0.693089)** is closer to a fair classification (compared to -6.356647 before).\n",
    "\n",
    "#### Performance on Balanced Dataset\n",
    "```\n",
    "Performance on Balanced Dataset (Training Time: 0.26 sec):\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      1.00      1.00     11411\n",
    "           1       1.00      0.99      0.99      5705\n",
    "\n",
    "    accuracy                           0.99     17116\n",
    "   macro avg       0.99      0.99      0.99     17116\n",
    "weighted avg       0.99      0.99      0.99     17116\n",
    "```\n",
    "\n",
    "#### Key Observations\n",
    "- **Accuracy = 0.99 (~99%)** → Still high, but more reliable now.\n",
    "- **Class 0 (Non-fraud cases)**:\n",
    "  - **Precision = 0.99, Recall = 1.00, F1-score = 1.00** → Almost perfect.\n",
    "- **Class 1 (Fraud cases)**:\n",
    "  - **Precision = 1.00** → Every fraud case predicted was actually fraud.\n",
    "  - **Recall = 0.99** → Almost every actual fraud case was detected.\n",
    "  - **F1-score = 0.99** → Balanced model performance.\n",
    "\n",
    "### 5.4. Why Balanced Data Performs Better\n",
    "- The **recall for fraud cases increased from 0.65 to 0.99** → The model is now detecting **almost all** fraud cases.\n",
    "- The **precision for fraud cases increased from 0.50 to 1.00** → Fewer false positives.\n",
    "- The **model generalizes better** because it learns from an equal distribution of both classes.\n",
    "- **False negative cases (missed fraud cases) are dramatically reduced**.\n",
    "\n",
    "\n",
    "\n",
    "### 5.5. Final Comparison: Imbalanced vs. Balanced\n",
    "| Metric        | Imbalanced Dataset | Balanced Dataset |\n",
    "|--------------|------------------|----------------|\n",
    "| **Accuracy** | **1.00** (Misleading) | **0.99** (More reliable) |\n",
    "| **Class 1 Precision (Fraud)** | **0.50** (50% of predicted fraud cases are correct) | **1.00** (No false positives) |\n",
    "| **Class 1 Recall (Fraud)** | **0.65** (Misses 35% of fraud cases) | **0.99** (Almost all fraud cases detected) |\n",
    "| **F1-score (Fraud)** | **0.57** | **0.99** |\n",
    "\n",
    "\n",
    "\n",
    "#### Conclusion\n",
    "- The **imbalanced dataset caused poor fraud detection**, with **low recall (0.65) and precision (0.50)**.\n",
    "- The **balanced dataset (via SMOTE or undersampling) improved model performance dramatically**.\n",
    "- **Handling class imbalance is crucial for fraud detection models**; otherwise, models will be **biased toward the majority class**.\n",
    "- **Recommendation:** Always **balance datasets** in classification problems where the minority class is underrepresented (e.g., fraud detection, rare disease classification)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.6. Confusion Matrices for Model Evaluation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:49:53.308829Z",
     "start_time": "2025-03-09T19:49:53.308829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Fraud\", \"Fraud\"], yticklabels=[\"No Fraud\", \"Fraud\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Confusion matrix for the imbalanced model\n",
    "plot_confusion_matrix(y_test, y_pred_lgb, \"Confusion Matrix - Imbalanced Dataset\")\n",
    "\n",
    "# Confusion matrix for the balanced model\n",
    "plot_confusion_matrix(y_test_bal, y_pred_bal_lgb, \"Confusion Matrix - Balanced Dataset\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "| **Actual / Predicted** | **No Fraud (0)** | **Fraud (1)** |\n",
    "|------------------------|-----------------|---------------|\n",
    "| **No Fraud (0)**       | **56800** (TN)  | **64** (FP)  |\n",
    "| **Fraud (1)**          | **34** (FN)     | **64** (TP)  |\n",
    "\n",
    "- **TN (True Negative: 56800)** → Model **correctly** predicted non-fraud cases.\n",
    "- **FP (False Positive: 64)** → Model **incorrectly** predicted fraud when it was actually non-fraud.\n",
    "- **FN (False Negative: 34)** → Model **missed 34 fraud cases**, predicting them as non-fraud.\n",
    "- **TP (True Positive: 64)** → Model **correctly** identified 64 fraud cases.\n",
    "\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "    - High Accuracy: The model performs well overall because most transactions are non-fraud.\n",
    "    - Poor Fraud Detection: Only 64 fraud cases detected, missing 34 fraud transactions.\n",
    "    - High Class Imbalance Impact: The model is biased toward non-fraud cases due to the imbalanced dataset.\n",
    "\n",
    "\n",
    "\n",
    "#### Confusion Matrix for the Balanced Dataset\n",
    "\n",
    "| **Actual / Predicted** | **No Fraud (0)** | **Fraud (1)** |\n",
    "|------------------------|------------------|---------------|\n",
    "| **No Fraud (0)**       | **11389** (TN)   | **22** (FP)   |\n",
    "| **Fraud (1)**          | **83** (FN)       | **5622** (TP)   |\n",
    "\n",
    "- **TN (True Negative: 11389)** → Model **correctly** predicted non-fraud cases.\n",
    "- **FP (False Positive: 22)** → Model **incorrectly** predicted fraud when it was actually non-fraud.\n",
    "- **FN (False Negative: 83)** → Model **missed 83 fraud cases**, predicting them as non-fraud.\n",
    "- **TP (True Positive: 5622)** → Model **correctly** identified 5622 fraud cases.\n",
    "\n",
    "#### Improvements After Balancing\n",
    "\n",
    "   - **Much Better Fraud Detection**: **5622 fraud cases** correctly detected, compared to only **64** before.\n",
    "   - **Lower False Positives**: The model only wrongly flagged **16** transactions as fraud.\n",
    "   - **Increased Sensitivity to Fraud**: The model now **detects more fraud cases** at the cost of a few extra false positives.\n",
    "\n",
    "#### Key Takeaways\n",
    "- The **imbalanced dataset** model **performs well in overall accuracy**, but **fails to detect fraud effectively**.\n",
    "- The **balanced dataset** model **detects fraud cases much better**, reducing **false negatives** (missed fraud cases).\n",
    "- **Balancing the dataset (e.g., SMOTE, undersampling)** **improves fraud detection**, making the model **more reliable**.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T19:49:53.311338Z",
     "start_time": "2025-03-09T19:49:53.310332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_precision_recall(y_true, y_probs, title):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Compare Precision-Recall Curves using returned probability predictions\n",
    "plot_precision_recall(y_test, y_probs_lgb, \"Precision-Recall Curve - Imbalanced Dataset\")\n",
    "plot_precision_recall(y_test_bal, y_probs_bal_lgb, \"Precision-Recall Curve - Balanced Dataset\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.7. Explanation of Precision-Recall (PR) Curves\n",
    "The **Precision-Recall (PR) Curve** helps evaluate the model's ability to **correctly detect fraud (positive class) while minimizing false positives**. It is particularly useful for **imbalanced datasets**, like our case of fraud detection.\n",
    "\n",
    "The two graphs compare the **imbalanced dataset** (before applying SMOTE/undersampling) and the **balanced dataset** (after applying resampling techniques).\n",
    "\n",
    "#### Precision-Recall Curve for the Imbalanced Dataset\n",
    "- The **curve starts at high precision (~1.0) but drops quickly** as recall increases.\n",
    "- This means that as the model tries to detect more fraud cases (**higher recall**), it **starts making a lot of false positives**, causing **precision to drop**.\n",
    "- The **rapid drop** suggests the model is **highly biased toward the majority class** (non-fraud), leading to **poor fraud detection**.\n",
    "- **Key Takeaway:** The model **fails to maintain good precision as recall increases**, indicating **poor fraud detection performance**.\n",
    "\n",
    "#### Precision-Recall Curve for the Balanced Dataset (Top Image)\n",
    "- The curve is **much more stable**, maintaining **high precision (~1.0) across most recall values**.\n",
    "- This suggests that the **model now correctly detects fraud cases without sacrificing precision**.\n",
    "- The **almost flat precision** at **high recall values** means the model **consistently identifies fraud cases correctly** with very few false positives.\n",
    "- **Key Takeaway:** **Balancing the dataset (e.g., SMOTE or undersampling) significantly improves fraud detection** by ensuring the model does not ignore minority-class fraud cases.\n",
    "\n",
    "\n",
    "### 5.8. Conclusion\n",
    "- **Imbalanced Dataset:** The model **struggles to detect fraud**. Precision drops quickly as it tries to detect more fraud cases.\n",
    "- **Balanced Dataset:** The model **performs much better**, maintaining **high precision** even at higher recall.\n",
    "- **Overall:** **Balancing the dataset helps improve fraud detection performance** by reducing false negatives (missed fraud) and keeping false positives under control."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Data Preprocessing and Model Training Pipeline\n",
    "This section provides an overview of the datasets used in this project. Each dataset presents **unique challenges, such as class imbalance and skewed distributions**, which are addressed using various preprocessing techniques.\n",
    "\n",
    "\n",
    "### 6.1. Schizophrenia Dataset\n",
    "#### Step 1: Load the Dataset\n",
    "Since `schizophrenia_dataset.csv` is different from `creditcard.csv`, we must load it while ensuring:\n",
    "- **The target column is correctly identified** (likely `\"Tanı\"` instead of `\"Class\"`).\n",
    "- **All numerical features are extracted properly**.\n",
    "- link: https://www.kaggle.com/datasets/asinow/schizohealth-dataset/data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the new dataset and define the target column\n",
    "target_column = 'Tanı'\n",
    "df, numerical_features = load_data(\"schizophrenia_dataset.csv\",target_column)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 2: Detect Skewed Features\n",
    "The dataset contains numerical features. We check for **highly skewed features** using the function we created earlier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Detect skewed numerical features\n",
    "skewed_features, skewness_table = detect_skewed_features(df, numerical_features)\n",
    "\n",
    "# Display the skewness table\n",
    "display(skewness_table)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 3: Transform Skewed Features\n",
    "To **normalize** the dataset and reduce skewness, we will apply the `best_transform()` function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Transform and store skewed features\n",
    "transformed_data, results_df = transform_and_store_skewed_features(df, skewed_features)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Display transformation results\n",
    "display(results_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following features were found to have significant skewness (>0.5):\n",
    "\n",
    "    - Hastalık_Süresi (Disease Duration)\n",
    "    - Hastaneye_Yatış_Sayısı (Hospital Admissions)\n",
    "    - Ailede_Şizofreni_Öyküsü (Family History of Schizophrenia)\n",
    "    - Madde_Kullanımı (Substance Use)\n",
    "    - İntihar_Girişimi (Suicide Attempts)\n",
    "    - Pozitif_Semptom_Skoru (Positive Symptom Score)\n",
    "    - Negatif_Semptom_Skoru (Negative Symptom Score)\n",
    "    - GAF_Skoru (Global Assessment of Functioning Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Visualize the Before & After Skewness"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot transformed distributions\n",
    "for data in transformed_data:\n",
    "    feature = data[\"feature\"]\n",
    "    skew_before = data[\"skew_before\"]\n",
    "    new_skew = data[\"skew_after\"]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))  # 2 Rows, 2 Columns\n",
    "\n",
    "    # Dynamically adjust bins based on range\n",
    "    data_range = np.ptp(data[\"original\"])  # Range of original data\n",
    "    bin_count = max(10, min(100, int(data_range / (np.std(data[\"original\"]) / 2))))  # Adjust bin size dynamically\n",
    "\n",
    "    # Histogram Before Transformation\n",
    "    sns.histplot(data[\"original\"], bins=bin_count, kde=True, ax=axes[0, 0], color='blue', alpha=0.6)\n",
    "    axes[0, 0].set_title(f'Before Transformation: {feature} (Skew: {skew_before:.2f})')\n",
    "\n",
    "    # Boxplot Before Transformation\n",
    "    sns.boxplot(x=data[\"original\"], ax=axes[0, 1], color='cyan')\n",
    "    axes[0, 1].set_title(f'Boxplot Before: {feature}')\n",
    "\n",
    "    # Dynamically adjust bins for transformed data\n",
    "    transformed_range = np.ptp(data[\"transformed\"])\n",
    "    transformed_bin_count = max(10, min(100, int(transformed_range / (np.std(data[\"transformed\"]) / 2))))\n",
    "\n",
    "    # Histogram After Transformation\n",
    "    sns.histplot(data[\"transformed\"], bins=transformed_bin_count, kde=True, ax=axes[1, 0], color='red', alpha=0.6)\n",
    "    axes[1, 0].set_title(f'After Transformation: {feature} (Skew: {new_skew:.2f})')\n",
    "\n",
    "    # Boxplot After Transformation\n",
    "    sns.boxplot(x=data[\"transformed\"], ax=axes[1, 1], color='orange')\n",
    "    axes[1, 1].set_title(f'Boxplot After: {feature}')\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this analysis, we applied transformations to **highly skewed numerical features** to improve their distribution. This skewness can negatively impact machine learning models, especially those that assume a normal distribution.\n",
    "The transformations, such as **log transformation, Box-Cox, or Yeo-Johnson**, helped to reduce skewness, normalize feature distributions, and minimize the impact of extreme outliers. \n",
    "\n",
    "The boxplots before and after transformation show a decrease in extreme values, making the data more balanced. These adjustments improve the model’s **accuracy, interpretability, and robustness**, as we mentioned before, particularly for algorithms that rely on evenly distributed data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Step 6: Apply TF-IDF\n",
    "\n",
    "We will use now the **TF-IDF transformation** function on categorical text columns to improve feature representation. To ensure this, we first check for textual data in the dataset."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Detect all text columns dynamically, excluding the target column\n",
    "text_columns = [col for col in df.select_dtypes(include=['object', 'string']).columns if col != target_column]\n",
    "\n",
    "if text_columns:\n",
    "    print(f\"\\nDetected text columns for TF-IDF transformation: {text_columns}\")\n",
    "else:\n",
    "    print(\"\\nNo text columns detected. Skipping TF-IDF transformation.\")\n",
    "\n",
    "# Convert text columns to string type\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "# Apply TF-IDF transformation only on detected text columns\n",
    "for col in text_columns:\n",
    "    df = apply_tfidf(df, col)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since no text columns were detected, we skip this step and proceed with other transformations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 5: Apply SMOTE and Under-sampling \n",
    "To address class imbalance, we apply **SMOTE** to generate synthetic samples for the minority class and **under-sampling** to reduce the majority class size. These techniques help create a more balanced dataset, improving model performance and reducing bias.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract features (X) and target (y)\n",
    "X = df[numerical_features]\n",
    "y = df[target_column]"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set the style\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Histogram\n",
    "sns.histplot(df[target_column], bins=30, kde=True, ax=axes[0], color='blue')\n",
    "axes[0].set_title('Schizophrenia Distribution - Histogram')\n",
    "axes[0].set_xlabel('Schizophrenia')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Boxplot\n",
    "sns.boxplot(x=df[target_column], ax=axes[1], color='green')\n",
    "axes[1].set_title('Schizophrenia Distribution - Boxplot')\n",
    "axes[1].set_xlabel('Schizophrenia')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The histogram and boxplot illustrate the distribution of the Schizophrenia target variable.\n",
    "\n",
    "- The **histogram** shows a highly **imbalanced dataset**, where most samples belong to one class (0 - no schizophrenia), with significantly fewer instances in the minority class (1 - schizophrenia).\n",
    "- The **boxplot** further confirms this imbalance, as it represents a binary classification with only two distinct values (0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply SMOTE & undersampling to balance classes\n",
    "X_smart, y_smart = auto_balance_pipeline(X, y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot class distribution before and after balancing\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.countplot(x=y, ax=axes[0])\n",
    "axes[0].set_title(\"Class Distribution Before Balancing\")\n",
    "axes[0].set_xlabel(\"Class\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "sns.countplot(x=y_smart, ax=axes[1])\n",
    "axes[1].set_title(\"Class Distribution After SMOTE + Undersampling\")\n",
    "axes[1].set_xlabel(\"Class\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The bar charts illustrate the **effect of applying SMOTE and undersampling** to balance the dataset:\n",
    "\n",
    "- Left (Before Balancing): The dataset was highly imbalanced, with significantly more samples in class 0 than in class 1. This imbalance could lead to biased model predictions.\n",
    "- Right (After Balancing): After applying SMOTE (oversampling the minority class) and undersampling the majority class, the dataset is now balanced, ensuring the model can learn from both classes equally.\n",
    "\n",
    "\n",
    "This final step improves **model performance, fairness, and generalization**, reducing bias toward the majority class. The pipeline is now ready for **training and evaluation** on a balanced dataset."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6.2. Dry Bean Dataset\n",
    "The **Dry Bean Dataset** contains morphological features of different bean types, which will be used for **classification tasks**.\n",
    "#### Step 1: Load the Dataset\n",
    "Since `Dry_Bean_Dataset.csv` differs from other datasets used, we need to load it carefully while ensuring:\n",
    "- **The target column is correctly identified** (in this case -`\"Class\"`).\n",
    "- **All numerical features are extracted properly** for analysis and preprocessing..\n",
    "- link: https://www.kaggle.com/datasets/muratkokludataset/dry-bean-dataset"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the new dataset\n",
    "target_column = 'Class'\n",
    "df, numerical_features = load_data(\"Dry_Bean_Dataset.csv\",target_column)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Step 2: Detect Skewed Features**\n",
    "As we can see the dataset contains numerical features. We check for **highly skewed features** using the function we created earlier."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Detect skewed numerical features\n",
    "skewed_features, skewness_table = detect_skewed_features(df, numerical_features)\n",
    "\n",
    "# Display the skewness table\n",
    "display(skewness_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Transform and store skewed features\n",
    "transformed_data, results_df = transform_and_store_skewed_features(df, skewed_features)\n",
    "print(results_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The table displays the results of **skewness correction** applied to numerical features. Before transformation, several features exhibited **high skewness**, which could negatively impact model performance.\n",
    "\n",
    "Transformation Methods Used:\n",
    "- Most features were transformed using the **Quantile transformation**, effectively reducing skewness to near zero.\n",
    "- The **Box-Cox** transformation was applied to ShapeFactor4, making it more normally distributed.\n",
    "\n",
    "After transformation, the skewness values are significantly reduced, ensuring a **more balanced and normalized dataset**, which helps improve model stability and performance."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot transformed distributions\n",
    "for data in transformed_data:\n",
    "    feature = data[\"feature\"]\n",
    "    skew_before = data[\"skew_before\"]\n",
    "    new_skew = data[\"skew_after\"]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))  # 2 Rows, 2 Columns\n",
    "\n",
    "    # Histogram Before Transformation\n",
    "    sns.histplot(data[\"original\"], bins=100, kde=True, ax=axes[0, 0], color='blue', alpha=0.6)\n",
    "    axes[0, 0].set_title(f'Before Transformation: {feature} (Skew: {skew_before:.2f})')\n",
    "\n",
    "    # Boxplot Before Transformation\n",
    "    sns.boxplot(x=data[\"original\"], ax=axes[0, 1], color='cyan')\n",
    "    axes[0, 1].set_title(f'Boxplot Before: {feature}')\n",
    "\n",
    "    # Histogram After Transformation\n",
    "    sns.histplot(data[\"transformed\"], bins=100, kde=True, ax=axes[1, 0], color='red', alpha=0.6)\n",
    "    axes[1, 0].set_title(f'After Transformation: {feature} (Skew: {new_skew:.2f})')\n",
    "\n",
    "    # Boxplot After Transformation\n",
    "    sns.boxplot(x=data[\"transformed\"], ax=axes[1, 1], color='orange')\n",
    "    axes[1, 1].set_title(f'Boxplot After: {feature}')\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "By applying **quantile or Box-Cox** transformations, we successfully normalize feature distributions, enhancing model performance and stability."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Step 6: Apply TF-IDF\n",
    "\n",
    "We will use now the **TF-IDF transformation** function on categorical text columns to improve feature representation. To ensure this, we first check for textual data in the dataset."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Detect all text columns dynamically, excluding the target column\n",
    "text_columns = [col for col in df.select_dtypes(include=['object', 'string']).columns if col != target_column]\n",
    "\n",
    "if text_columns:\n",
    "    print(f\"\\n Detected text columns for TF-IDF transformation: {text_columns}\")\n",
    "else:\n",
    "    print(\"\\n No text columns detected. Skipping TF-IDF transformation.\")\n",
    "\n",
    "# Convert text columns to string type\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "# Apply TF-IDF transformation only on detected text columns\n",
    "for col in text_columns:\n",
    "    df = apply_tfidf(df, col)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since no text columns were detected, we skip this step."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 5: Apply SMOTE and Under-sampling \n",
    "To address class imbalance, we apply **SMOTE** to generate synthetic samples for the minority class and **under-sampling** to reduce the majority class size. These techniques help create a more balanced dataset, improving model performance and reducing bias.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract features (X) and target (y)\n",
    "X = df[numerical_features]\n",
    "y = df[target_column]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set the style\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Create the figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Histogram with KDE\n",
    "sns.histplot(df[target_column], bins=30, kde=True, color='blue')\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Class Distribution - Histogram')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The plot illustrate the distribution of price across different bean types:\n",
    "- The distribution varies significantly across categories, with some bean types (e.g., Dermason) having noticeably higher frequency.\n",
    "- Certain categories (e.g., Bombay) have fewer occurrences, indicating an uneven distribution.\n",
    "\n",
    "Understanding price distribution helps in feature selection, model training, and identifying potential biases in the dataset."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply SMOTE & undersampling to balance classes\n",
    "X_smart, y_smart = auto_balance_pipeline(X, y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot class distribution before and after balancing\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.countplot(x=y, ax=axes[0])\n",
    "axes[0].set_title(\"Class Distribution Before Balancing\")\n",
    "axes[0].set_xlabel(\"Class\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "sns.countplot(x=y_smart, ax=axes[1])\n",
    "axes[1].set_title(\"Class Distribution After SMOTE + Undersampling\")\n",
    "axes[1].set_xlabel(\"Class\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The bar charts illustrate the effect of applying **SMOTE and undersampling** to balance the dataset:\n",
    "\n",
    "- Left (Before Balancing): The dataset was highly imbalanced, with certain bean types (e.g., Dermason, Sira) having significantly more samples than others (e.g., Bombay). This imbalance could lead to biased model predictions.\n",
    "- Right (After Balancing): After applying SMOTE (oversampling minority classes) and undersampling majority classes, the dataset is now balanced, ensuring the model can learn from all classes equally.\n",
    "\n",
    "This final step improves **model performance, fairness, and generalization**, reducing bias toward dominant classes. The pipeline is now ready for **training and evaluation** on a balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.3. Adult Income Dataset\n",
    "#### Step 1: Load the Dataset\n",
    "Again, we load the dataset using the load function, ensuring the correct target column is selected and numerical features are properly extracted.\n",
    "- **The target column is correctly identified** (in this case -`\"income\"`).\n",
    "- **All numerical features are extracted properly** for analysis and preprocessing..\n",
    "- link: https://www.kaggle.com/datasets/wenruliu/adult-income-dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the new dataset\n",
    "target_column = 'income'\n",
    "df, numerical_features = load_data(\"adult.csv\",target_column)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 2: Detect Skewed Features\n",
    "As we can see the dataset contains numerical features. We check for **highly skewed features** using the function we created earlier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Detect skewed numerical features\n",
    "skewed_features, skewness_table = detect_skewed_features(df, numerical_features)\n",
    "\n",
    "# Display the skewness table\n",
    "display(skewness_table)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 3: Transform Skewed Features\n",
    "To **normalize** the dataset and reduce skewness, we will apply the `best_transform()` function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Transform and store skewed features\n",
    "transformed_data, results_df = transform_and_store_skewed_features(df, skewed_features)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Display transformation results\n",
    "display(results_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 4: Visualize the Before & After Skewness\n",
    "We will use the same approach as we have done on the first dataset to visualize the before and after skewness."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot transformed distributions\n",
    "for data in transformed_data:\n",
    "    feature = data[\"feature\"]\n",
    "    skew_before = data[\"skew_before\"]\n",
    "    new_skew = data[\"skew_after\"]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))  # 2 Rows, 2 Columns\n",
    "\n",
    "    # Histogram Before Transformation\n",
    "    sns.histplot(data[\"original\"], bins=100, kde=True, ax=axes[0, 0], color='blue', alpha=0.6)\n",
    "    axes[0, 0].set_title(f'Before Transformation: {feature} (Skew: {skew_before:.2f})')\n",
    "\n",
    "    # Boxplot Before Transformation\n",
    "    sns.boxplot(x=data[\"original\"], ax=axes[0, 1], color='cyan')\n",
    "    axes[0, 1].set_title(f'Boxplot Before: {feature}')\n",
    "\n",
    "    # Histogram After Transformation\n",
    "    sns.histplot(data[\"transformed\"], bins=100, kde=True, ax=axes[1, 0], color='red', alpha=0.6)\n",
    "    axes[1, 0].set_title(f'After Transformation: {feature} (Skew: {new_skew:.2f})')\n",
    "\n",
    "    # Boxplot After Transformation\n",
    "    sns.boxplot(x=data[\"transformed\"], ax=axes[1, 1], color='orange')\n",
    "    axes[1, 1].set_title(f'Boxplot After: {feature}')\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Step 6: Apply TF-IDF\n",
    "\n",
    "We will use now the **TF-IDF transformation** function on categorical text columns to improve feature representation. This approach helps handle **high-cardinality features** like `occupation` and `native-country`, reducing sparsity while retaining meaningful distinctions between categories."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Detect all text columns dynamically, excluding the target column\n",
    "text_columns = [col for col in df.select_dtypes(include=['object', 'string']).columns if col != target_column]\n",
    "\n",
    "if text_columns:\n",
    "    print(f\"\\nDetected text columns for TF-IDF transformation: {text_columns}\")\n",
    "else:\n",
    "    print(\"\\nNo text columns detected. Skipping TF-IDF transformation.\")\n",
    "\n",
    "# Convert text columns to string type\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "# Apply TF-IDF transformation only on detected text columns\n",
    "for col in text_columns:\n",
    "    df = apply_tfidf(df, col)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**The TF-IDF (Term Frequency-Inverse Document Frequency)** transformation was applied to categorical text columns to convert them into **numerical features** for machine learning models. The transformation helps represent text data in a meaningful way while reducing the impact of frequently occurring words.\n",
    "\n",
    "    - Total transformed columns: 8\n",
    "    - Number of new features added per column:\n",
    "    - workclass: 13\n",
    "    - education: 29\n",
    "    - marital-status: 14\n",
    "    - occupation: 41\n",
    "    - relationship: 6\n",
    "    - race: 12\n",
    "    - gender: 2\n",
    "    - native-country: 57\n",
    "    \n",
    "**Final dataset shape**: (48842, 181) → The dataset has expanded as new features were generated.\n",
    "\n",
    "\n",
    "**Visualization: Bar Chart of Feature Expansion**\n",
    "\n",
    "Show the **number of TF-IDF features extracted per column**, illustrating which categorical variables contributed the most new features."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns = ['workclass', 'education', 'marital-status', 'occupation', \n",
    "           'relationship', 'race', 'gender', 'native-country']\n",
    "tfidf_features = [13, 29, 14, 41, 6, 12, 2, 57]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(columns, tfidf_features, color='blue')\n",
    "plt.xlabel(\"Number of TF-IDF Features\")\n",
    "plt.ylabel(\"Categorical Columns\")\n",
    "plt.title(\"TF-IDF Feature Expansion per Column\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The bar chart illustrates the **number of new features** generated for each categorical column using TF-IDF transformation.\n",
    "\n",
    "Columns with the most TF-IDF features:\n",
    "- **Native-country (57 features)** and **occupation (41 features)** have the highest expansion, indicating a large variety of unique values in these categories.\n",
    "\n",
    "Columns with the least TF-IDF features:\n",
    "- **Gender (2 features)** and **relationship (6 features)** contributed the fewest new features, suggesting a low diversity of unique values.\n",
    "\n",
    "This transformation converts categorical text data into **numerical representations**, allowing models to leverage textual information effectively. The number of features per column reflects the complexity and diversity of categorical values in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 5: Apply SMOTE and Under-sampling\n",
    "To address class imbalance, we apply our **SMOTE** function to generate synthetic samples for the minority class and **undersampling** to reduce the majority class. This balanced approach enhances model performance by preventing bias toward the dominant class while preserving meaningful patterns in the data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract features (X) and target (y)\n",
    "X = df[numerical_features]\n",
    "y = df[target_column]"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply SMOTE & undersampling to balance classes\n",
    "X_smart, y_smart = auto_balance_pipeline(X, y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the class distribution was **imbalanced before SMOTE** (`<=50K`: 37,155, `>50K`: 11,687). After applying **SMOTE**, the distribution is now more balanced (`<=50K`: 37,155, `>50K`: 26,008). This helps our model **learn better from both classes** and reduces bias toward the majority class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we can see that before undersampling, the class distribution was **slightly imbalanced** (`<=50K`: 37,155, `>50K`: 26,008) with a **1.43:1 ratio**. After applying **light undersampling**, the distribution remains unchanged as the imbalance was within an acceptable range. This ensures we retain sufficient data while preventing unnecessary loss of valuable information."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot class distribution before and after balancing\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.countplot(x=y, ax=axes[0])\n",
    "axes[0].set_title(\"Class Distribution Before Balancing\")\n",
    "axes[0].set_xlabel(\"Class\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "sns.countplot(x=y_smart, ax=axes[1])\n",
    "axes[1].set_title(\"Class Distribution After SMOTE + Undersampling\")\n",
    "axes[1].set_xlabel(\"Class\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The plot illustrates the **class distribution before and after balancing**. Initially, the dataset was highly imbalanced, with significantly fewer instances of the `>50K` class. After applying **SMOTE and undersampling**, the distribution is more balanced, ensuring the model learns effectively from both classes and reducing bias toward the majority class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Conclusion\n",
    "This project addressed the challenges of handling skewed and imbalanced datasets in machine learning. Imbalanced data often leads to biased models that favor the majority class, resulting in suboptimal performance for minority class predictions.\n",
    "\n",
    "To overcome these issues, we developed a **flexible pipeline** that integrates **data preprocessing, model optimization, and performance evaluation**. The pipeline incorporates various **data balancing techniques** such as **SMOTE, ADASYN, and under-sampling**, along with **data transformation methods like Quantile Transformation and Power Transformation** to enhance feature distributions. Additionally, we leveraged **advanced machine learning models, including LightGBM**, to optimize predictive performance.\n",
    "\n",
    "The results demonstrate that a well-structured approach combining data transformation and balancing techniques can significantly improve model performance on imbalanced datasets. This work provides a solid foundation for future research and practical applications in domains where data imbalance is a prevalent challenge.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
